[
  {
    "objectID": "blog/ggplot2_book_3e_part1.html",
    "href": "blog/ggplot2_book_3e_part1.html",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "",
    "text": "In this series of posts, I will be completing the exercises in ggplot2: Elegant Graphics for Data Analysis (3e), the ultimate guide to {ggplot2}. I wanted to practice this textbook to better my knowledge of {ggplot2}, but also get a feel for the design behind the package, The Grammar of Graphics.\n\n“Without a grammar, there is no underlying theory, so most graphics packages are a big collection of special cases.”\n\nI will not be re-iterating all of the information from the book, but provide a brief summary of each section and run through the exercises. Follow along to see my take on the exercises, as well as my notes and thoughts as I progress through the book.\nIf you would like to see the source code behind this post, you can click on the Code button at the top of right of the page, sandwiched between the title of the post and the side panel.\nThe book is split into five parts: Getting Started, Layers, Scales, The Grammar, & Advanced Topics. In this post, I will be working through the first part, Getting Started.\n\n\n\n\n\n\nTip\n\n\n\nBecause this is book about {ggplot2}, I will use package-explicit function when using a function for the first time that is not in base R or provided by {ggplot2}. All of these packages are loaded with the {tidyverse} meta-package.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe book was still in development when writing this post, so some exercises might not match depending on when you are reading of this post.\n\n\n\n\ninstall.packages(c(\n  \"colorBlindness\", \"directlabels\", \"dplyr\", \"ggforce\", \"gghighlight\",\n  \"ggnewscale\", \"ggplot2\", \"ggraph\", \"ggtext\", \"ggthemes\", \"hexbin\", \"Hmisc\", \n  \"mapproj\", \"maps\", \"munsell\", \"ozmaps\", \"paletteer\", \"patchwork\", \"rmapshaper\",\n  \"scico\", \"seriation\", \"sf\", \"stars\", \"tidygraph\", \"tidyr\", \"wesanderson\"\n))"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#required-packages",
    "href": "blog/ggplot2_book_3e_part1.html#required-packages",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "",
    "text": "install.packages(c(\n  \"colorBlindness\", \"directlabels\", \"dplyr\", \"ggforce\", \"gghighlight\",\n  \"ggnewscale\", \"ggplot2\", \"ggraph\", \"ggtext\", \"ggthemes\", \"hexbin\", \"Hmisc\", \n  \"mapproj\", \"maps\", \"munsell\", \"ozmaps\", \"paletteer\", \"patchwork\", \"rmapshaper\",\n  \"scico\", \"seriation\", \"sf\", \"stars\", \"tidygraph\", \"tidyr\", \"wesanderson\"\n))"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#introduction-1",
    "href": "blog/ggplot2_book_3e_part1.html#introduction-1",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\nThe goal of this chapter it so introduce the reader to {ggplot2} as quickly as possible. Because it’s an intro, I will not be formatting the plots any further than the questions ask for."
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#fuel-economy-data",
    "href": "blog/ggplot2_book_3e_part1.html#fuel-economy-data",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.2 Fuel Economy Data",
    "text": "2.2 Fuel Economy Data\nIn this chapter, we will be using mostly one data set, mpg, from http://fueleconomy.gov. It holds information about the fuel economy of popular car models in 1999 & 2009.\n\nlibrary(tidyverse) # Data Wrangling, includes {ggplot2}\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nA quick overview of the variables:\n\n\ncty and hwy record miles per gallon (mpg) for city and highway driving.\n\ndispl is the engine displacement in liters.\n\ndrv is the drivetrain: front wheel (f), rear wheel (r), or four wheel.\n\nmodel is the model of car. There are 38 models, selected because they had a new edition every year between 1999 and 2008.\n\nclass is a categorical variable describing the “type” of car: two seater, SUV, compact, etc.\n\n\n2.2.1 Exercises\n1. List five functions that you could use to get more information about the mpg dataset.\n\nhelp(mpg)\nglimpse(mpg)\nhead(mpg)\nstr(mpg)\nView(mpg)\n\n2. How can you find out what other datasets are included with {ggplot2}?\n\ndata(package = 'ggplot2')\n\n3. Apart from the US, most countries use fuel consumption (fuel consumed over fixed distance) rather than fuel economy (distance traveled with fixed amount of fuel). How could you convert cty and hwy into the European standard of l/100km?\n\nus_to_euro = function(mpg) {\n  # 1 mile = 1.60934 kilometers\n  # 1 gallon (US) = 3.78541 liters\n  \n  g_p_m = 1 / mpg\n  l_p_km = 3.78541 / 1.60934    # we multiply by 100 because it's \"per 100\"\n  l100km = l_p_km * 100 * g_p_m # the denominator 100 cancels right hand 100\n  \n  return(l100km)\n}\n\nmpg |&gt; dplyr::mutate(\n  cty_euro = us_to_euro(cty), \n  hwy_euro = us_to_euro(hwy), \n  .keep = 'used')\n\n# A tibble: 234 × 4\n     cty   hwy cty_euro hwy_euro\n   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1    18    29     13.1     8.11\n 2    21    29     11.2     8.11\n 3    20    31     11.8     7.59\n 4    21    30     11.2     7.84\n 5    16    26     14.7     9.05\n 6    18    26     13.1     9.05\n 7    18    27     13.1     8.71\n 8    18    26     13.1     9.05\n 9    16    25     14.7     9.41\n10    20    28     11.8     8.40\n# ℹ 224 more rows\n\n\n4. Which manufacturer has the most models in this dataset? Which model has the most variations? Does your answer change if you remove the redundant specification of drivetrain (e.g. “pathfinder 4wd”, “a4 quattro”) from the model name?\nmpg |&gt; dplyr::count(manufacturer)\nmpg |&gt; count(model)\nmpg |&gt; \n  mutate(model_base = stringr::str_extract(model, \"^\\\\w+\")) |&gt; \n  count(model_base) # yes\n\n\n\n# A tibble: 15 × 2\n   manufacturer     n\n   &lt;chr&gt;        &lt;int&gt;\n 1 audi            18\n 2 chevrolet       19\n 3 dodge           37\n 4 ford            25\n 5 honda            9\n 6 hyundai         14\n 7 jeep             8\n 8 land rover       4\n 9 lincoln          3\n10 mercury          4\n11 nissan          13\n12 pontiac          5\n13 subaru          14\n14 toyota          34\n15 volkswagen      27\n\n\n# A tibble: 38 × 2\n   model                  n\n   &lt;chr&gt;              &lt;int&gt;\n 1 4runner 4wd            6\n 2 a4                     7\n 3 a4 quattro             8\n 4 a6 quattro             3\n 5 altima                 6\n 6 c1500 suburban 2wd     5\n 7 camry                  7\n 8 camry solara           7\n 9 caravan 2wd           11\n10 civic                  9\n# ℹ 28 more rows\n\n\n# A tibble: 35 × 2\n   model_base     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 4runner        6\n 2 a4            15\n 3 a6             3\n 4 altima         6\n 5 c1500          5\n 6 camry         14\n 7 caravan       11\n 8 civic          9\n 9 corolla        5\n10 corvette       5\n# ℹ 25 more rows"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#key-components",
    "href": "blog/ggplot2_book_3e_part1.html#key-components",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.3 Key Components",
    "text": "2.3 Key Components\nEvery {ggplot2} plot has three key components:\n\n\nData,\nA set of aesthetic mappings between variables in the data and visual properties, and\nAt least one layer which describes how to render each observation. Layers are usually created with a geom function.\n\nHere’s a simple example:\n\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()\n\n\n\n\n\n\n\n\n2.3.1 Exercises\n1. How would you describe the relationship between cty and hwy? Do you have any concerns about drawing conclusions from that plot?\nThere is a strong positive linear relationship between city & highway gas mileage. Just plotting only those two might generalize too much across different classes of vehicles. Even though it may be true, maybe different classes of vehicles are more equal in city vs highway gas mileage vs performing substantially better in one or the other.\n2. What does ggplot(mpg, aes(model, manufacturer)) + geom_point() show? Is it useful? How could you modify that data to make it more informative?\n\nggplot(mpg, aes(model, manufacturer)) + geom_point()\n\n\n\n\n\n\n\nThis plot just shows which manufacturers make which models. Having two categorical variables on a dot plot is not very useful as there is no inherent value in the relationship between two categories existing. Turning one category into a count() or other stat would show a dimensional relationship across the other category.\n\nmpg |&gt; count(manufacturer) |&gt; ggplot(aes(n, manufacturer)) + geom_point()\n\n\n\n\n\n\n\n3. Describe the data, aesthetic mappings, and layers used for each of the following plots. You’ll need to guess a little because you haven’t seen all the datasets and functions yet, but use your common sense! See if you can predict what the plot will look like before running the code.\n\n\nggplot(mpg, aes(cty, hwy)) + geom_point() A dot plot showing a positive relationship between city mpg and highway mpg.\n\nggplot(diamonds, aes(carat, price)) + geom_point() A dot plot showing a positive relationship between diamond price and its carat rating.\n\nggplot(economics, aes(date, unemploy)) + geom_line() A line plot showing unemployment rate across time.\n\nggplot(mpg, aes(cty)) + geom_histogram() A histogram showing the distribution of cars across city mpg rating.\n\nggplot(mpg, aes(cty, hwy)) + geom_point()\nggplot(diamonds, aes(carat, price)) + geom_point()\nggplot(economics, aes(date, unemploy)) + geom_line()\nggplot(mpg, aes(cty)) + geom_histogram()"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#color-size-shape-and-other-aesthetic-attributes",
    "href": "blog/ggplot2_book_3e_part1.html#color-size-shape-and-other-aesthetic-attributes",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.4 Color, Size, Shape, and Other Aesthetic Attributes",
    "text": "2.4 Color, Size, Shape, and Other Aesthetic Attributes\nTo add additional variables to a plot, we can use other aesthetics like color, shape, and size. These work in the same way as the x and y aesthetics, and are added into the call to aes():\n\naes(displ, hwy, color = class)\naes(displ, hwy, shape = drv)\naes(displ, hwy, size = cyl)\n\n\nggplot(mpg, aes(displ, hwy, color = class)) + geom_point()\n\n\n\n\n\n\n\n\n2.4.1 Exercises\n1. Experiment with the color, shape and size aesthetics. What happens when you map them to continuous values? What about categorical values? What happens when you use more than one aesthetic in a plot?\nggplot(mpg, aes(displ, hwy, color = cyl)) + geom_point()\nggplot(mpg, aes(displ, hwy, color = as.character(year))) + geom_point()\nggplot(mpg, aes(displ, hwy, color = cyl, shape = as.character(year))) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. What happens if you map a continuous variable to shape? Why? What happens if you map trans to shape? Why?\n\nggplot(mpg, aes(displ, hwy, shape = hwy)) + geom_point()\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `scale_f()`:\n! A continuous variable cannot be mapped to the shape aesthetic.\nℹ Choose a different aesthetic or use `scale_shape_binned()`.\n\n\nYou get an error because continuous variables lie on a scale of infinity, and you cannot have infinite shapes. This is why in the previous question, converter year into a character because it is a continuous variable year = 1999 in the data frame, but its use is actually as a category, comparing 1999 vehicles to 2008 vehicles.\n\nggplot(mpg, aes(displ, hwy, shape = trans)) + geom_point()\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 10 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\nWarning: Removed 96 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nYou get a warning because although trans is a categorical variable, it has more values than {ggplot2} has shapes (6 in total), so other values do not get markers.\nThis highlights the difference between Errors and Warnings with {ggplot2}. As with regular R code, Warnings show where the code can still run but probably with not the effect that was intended, where Errors are impossible to process and the code does not run.\n3. How is drive train related to fuel economy? How is drive train related to engine size and class?\nggplot(mpg, aes(drv, hwy)) + geom_point()\nggplot(mpg, aes(displ, class, color = drv)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nFront-wheel transmission vehicle seems to have the best highway gas mileage, where 4-wheel and rear-wheel show similar performance to each other.\nAlmost all 4-wheel drive vehicles (in this dataset) are either an SUV or Pickup, and have the biggest range in engine size. The smallest vehicles (2-seater & subcompact) have bigger engines and are rear-wheel drive, probably sports cars of some sort. Finally, the regular everyday vehicles like compact & midsize cars have smaller engines and mostly front-wheel drive transmissions."
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#faceting",
    "href": "blog/ggplot2_book_3e_part1.html#faceting",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.5 Faceting",
    "text": "2.5 Faceting\nFaceting creates tables of graphics by splitting the data into subsets and displaying the same graph for each subset. The two type so faceting are grid and wrapped. We will be focusing on wrapped.\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~class)\n\n\n\n\n\n\n\n\n2.5.1 Exercises\n1. What happens if you try to facet by a continuous variable like hwy? What about cyl? What’s the key difference?\nggplot(mpg, aes(displ, cty)) + geom_point() + facet_wrap(~hwy)\nggplot(mpg, aes(displ, cty)) + geom_point() + facet_wrap(~cyl)\n\n\n\n\n\n\n\n\n\n\nThey both facet by the number of unique values in the variable. In the case of hwy, there were 27 unique values in this limited dataset because it truly represents a continuous variable. This makes it a bad choice for faceting.\ncyl on the other hand, actually represents a category (# of cylinders in an engine) although it’s in the data frame as a continuous. This may be because it is a number, which is usually continuous.\n2. Use faceting to explore the 3-way relationship between fuel economy, engine size, and number of cylinders. How does faceting by number of cylinders change your assessment of the relationship between engine size and fuel economy?\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~cyl)\n\n\n\n\n\n\n\nFaceting by cylinders shows a clearly that smaller engines perform better on fuel economy vs their bigger counterparts, though being a smaller engine does not necessarily mean that it will have good gas mileage.\n3. Read the documentation for facet_wrap(). What arguments can you use to control how many rows and columns appear in the output?\nnrow & ncol are the arguments to control the number of rows & columns. Here is an extreme example:\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~trans, nrow = 1)\n\n\n\n\n\n\n\n4. What does the scales argument to facet_wrap() do? When might you use it?\nBy default, the scales locks both the x and y scales on all faceted plots to show the same range, regardless of the range of values in each facet. You might want to free a scale if the axis doesn’t have any values for that facet, and the missing range doesn’t affect the analysis.\nIn this example, we don’t need x-axis values for vehicles that don’t exist, but keeping the y-axis values on the same scale helps to compare the values across all manufacturers.\n\nmpg |&gt; \n  ggplot(aes(as_factor(cyl), hwy)) + \n  geom_point() + \n  facet_wrap(~manufacturer, scales = \"free_x\", nrow = 3)"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#plot-geoms",
    "href": "blog/ggplot2_book_3e_part1.html#plot-geoms",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.6 Plot Geoms",
    "text": "2.6 Plot Geoms\nSubstituting geom_point() for a different geom function creates a different plot. Who would’ve thought? In the following sections, we will cover some of the other most used geoms provided in {ggplot2}:\n\n\ngeom_smooth() fits a smoother to the data and displays the smooth and its standard error.\n\ngeom_boxplot() produces a box-and-whisker plot to summarize the distribution of set of points.\n\ngeom_histogram() and geom_freqpoly() show the distribution of continuous variables.\n\ngeom_bar() shows the distribution of categorical variables.\n\ngeom_path() and geom_line() draw lines between the data points. A line plot is constrained to produce lines that travel from left to right, while paths can go in any direction. Lines are typically used to explore how things change over time.\n\n\n2.6.1 Adding a smoother to a plot\nIf you have a scatterplot with a lot of noise, it can be hard to see the dominant pattern. In this case, it’s useful to add a smoothed line to the plot with geom_smooth():\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAn important argument to geom_smooth() is the method, which allows you to choose which type of model is used to fit the smooth curve:\n\n\nmethod = \"loess\", the default for small n, uses a smooth local regression. \"span\" controls the level of smoothing.\n\nmethod = \"gam\" fits a generalized additive model provided by the {mgcv} package. You need to load in the package then use a formula = y ~ s(x) or y ~ s(x, bs = \"cs\") (for large data).\n\nmethod = \"lm\" fits a linear model, giving the line of best fit.\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(span = 0.2)\nggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(span = 1)\nlibrary(mgcv)\nggplot(mpg, aes(displ, hwy)) + \n  geom_point() + \n  geom_smooth(method = \"gam\", formula = y ~ s(x))\nggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 Boxplots and jittered points\nWhen a dataset contains a categorical variable and one or more continuous variables, we might be interested in the distribution of the continuous variable(s) relative to the categorical variable. Because there a few unique number of values for both drv and hwy, there is a lot of overplotting. There are a few useful techniques to help with this issue:\nggplot(mpg, aes(drv, hwy)) + geom_point()\nggplot(mpg, aes(drv, hwy)) + geom_jitter()\nggplot(mpg, aes(drv, hwy)) + geom_boxplot()\nggplot(mpg, aes(drv, hwy)) + geom_violin()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThough these are useful techniques, they also have their own drawbacks.\n\n\n\n\n\n\n\nJitter Plots\nBoxplots\nViolin Plots\n\n\n\nAdd a little random noise to the data which can help avoid overplotting.\nSummarize the shape of the distribution with a handful of summary statistics.\nShow a compact representation of the “density” of the distribution, highlighting the areas where more points are found.\n\n\nShow every point but only work with relatively small datasets.\nSummarize the whole distribution with 5 statistics.\nGive the richest display, but the density estimate can be hard to interpret.\n\n\n\n2.6.3 Histograms and frequency polygons\nHistograms and frequency polygons show the distribution of a single numeric variable with more detail than a boxplot but at the expense of needing more space. The only difference between the two is that the prior uses columns and the latter uses lines.\nggplot(mpg, aes(hwy)) + geom_histogram()\nggplot(mpg, aes(hwy)) + geom_freqpoly()\n\n\n\n\n\n\n\n\n\n\nIt is highly recommended to experiment with the bins as the default value is 30 and it is unlikely that 30 is the best choice for your dataset.\nggplot(mpg, aes(hwy)) + geom_freqpoly(bins = 15)\nggplot(mpg, aes(hwy)) + geom_freqpoly(bins = 45)\n\n\n\n\n\n\n\n\n\n\nTo compare distributions of subgroups, you can map a categorical variable to either fill (for geom_histogram()) or color (for geom_freqpoly()).\nggplot(mpg, aes(displ, colour = drv)) + \n  geom_freqpoly(binwidth = 0.5)\nggplot(mpg, aes(displ, fill = drv)) + \n  geom_histogram(binwidth = 0.5) + \n  facet_wrap(~drv, ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n2.6.4 Bar charts\nThe discrete analogue of the histogram is the bar chart, geom_bar().\n\nggplot(mpg, aes(manufacturer)) + geom_bar()\n\n\n\n\n\n\n\nBar charts can be confusing because there are two very different plots that are both commonly called bar charts.\n\nThe first form, like above, assumes your data is not summarized, and each observation contributes to one unit to the height of each bar.\nThe second form is used for pre-summarized data.\n\nFor example, you might have three drugs with the their average effect. To display this type of data, you have to tell geom_bar() to not run the default stat which bins and counts data. In this case, it’s better to use geom_point() because it takes up less space than bars, and don’t require that the axis includes 0.\ndrugs = tibble(drug = c(\"a\", \"b\", \"c\"), effect = c(4.2, 9.7, 6.1))\n\nggplot(drugs, aes(drug, effect)) + geom_bar(stat = \"identity\")\nggplot(drugs, aes(drug, effect)) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n2.6.4.1 Bonus\nEven if using geom_point() might be preferred, because the second type of bar/column chart is so popular, {ggplot2} includes a geom_col() that acts exactly the same as geom_bar(stat = \"identity\"):\nggplot(drugs, aes(drug, effect)) + geom_bar(stat = \"identity\")\nggplot(drugs, aes(drug, effect)) + geom_col()\n\n\n\n\n\n\n\n\n\n\n\n2.6.5 Time series with line and path plots\nLine and path plots are typically used for time series data, where the order of the data matters to the context of the visual. Line plots join the data points from left to right, while path plots join the points in the order that they appear in the dataset.\n\n\n\n\n\n\nLine Plot\nPath Plot\n\n\n\nPlots the data from left to right.\nPlots the points in the order they appear in dataset.\n\n\nShow time on x-axis.\nShow how two variables simultaneously change over time, with time encoded in the way the data points are connected.\n\n\n\nThe two plots below show unemployment over time, both with geom_line(). The firsts shows unemployment rate while the second shows the median number of weeks unemployed.\nggplot(economics, aes(date, unemploy / pop)) + geom_line()\nggplot(economics, aes(date, uempmed)) + geom_line()\n\n\n\n\n\n\n\n\n\n\nTo compare the relationship, we would like to draw both time series on the same plot. We could draw a scatterplot of unemployment rate vs length of time unemployed, but then we lose the dimension of time. The solution is to join points adjacent in time with line segments, forming a path plot.\nggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path() +\n  geom_point()\nggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path(color = \"grey50\") +\n  geom_point(aes(color = lubridate::year(date)))\n\n\n\n\n\n\n\n\n\n\n\n2.6.6 Exercises\n\n\n\n\n\n\nNote\n\n\n\nGoing through the exercises of this section, they don’t all completely line up with content above, so I believe this part is still a WIP. Regardless, I tried to answered the questions to the best of what I think the exercises are going for.\n\n\n1. What’s the problem with the plot created by ggplot(mpg, aes(cty, hwy)) + geom_point()? Which of the geoms described above is most effective at remedying the problem?\nBecause the two values are so highly correlated, there might be some overplotting in the plot. We can check for overplotting imonn a scatterplot by adjust the alpha value of the points.\nggplot(mpg, aes(cty, hwy)) + geom_point()\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n2. One challenge with ggplot(mpg, aes(class, hwy)) + geom_boxplot() is that the ordering of class is alphabetical, which is not terribly useful. How could you change the factor levels to be more informative? Rather than reordering the factor by hand, you can do it automatically based on the data: ggplot(mpg, aes(reorder(class, hwy), hwy)) + geom_boxplot(). What does reorder() do? Read the documentation.\nggplot(mpg, aes(class, hwy)) + geom_boxplot()\nggplot(mpg, aes(\n  x = factor(class, levels = c(\n    \"pickup\", \"suv\", \"minivan\", \"2seater\", \"subcompact\", \"compact\", \"midsize\")), \n  y = hwy)) + \n  geom_boxplot()\nggplot(mpg, aes(reorder(class, hwy), hwy)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nreorder() treats its first argument as a categorical variable, and reorders its levels based on the values of a second variable, usually numeric. Comparing my manual reordering (based on the median line of the boxplots), reorder() takes a different approach that doesn’t look apparently clear from the boxplot.\n3. Explore the distribution of the carat variable in the diamonds dataset. What binwidth reveals the most interesting patterns?\nYou can use either bin or binwidth to distribute the columns of a histogram. You can test bin values arbitrarily until you find a plot that looks good, but using the binwidth argument, you can make educated guess as to where to begin and go because the value is directly related to the value/scaling of the x-axis.\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 1/10)\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 1/50) \nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 1/100)\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 1/500)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing binwidth values of 1/50 & 1/100 shows that the caret values heavily sit on half and quarter caret values, skewing right until it breaks again.\n4. Explore the distribution of the price variable in the diamonds data. How does the distribution vary by cut?\nggplot(diamonds, aes(price)) + geom_freqpoly()\nggplot(diamonds, aes(price)) + geom_freqpoly(aes(color = cut))\n\n\n\n\n\n\n\n\n\n\nThe regular and segmented plots both show a similar distribution. I’m not surprised that higher value cuts sell more, but I am surprised that all the cuts sell the most at the same price range. I would’ve thought that higher value cuts would seller at higher prices more frequently.\n5. You now know (at least) three ways to compare the distributions of subgroups: geom_violin(), geom_freqpoly(), and the color aesthetic, or geom_histogram() and faceting. What are the strengths and weaknesses of each approach? What other approaches could you try?\nThe strengths and weakness were outlined pretty well above, but you could also try adjust with the transparency of the data points, which also can be used with two sets of continuous variables.\nggplot(mpg, aes(drv, hwy)) + geom_point(alpha = 0.2) # Category vs Continuous\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 0.2) # Continuous vs Continuous\n\n\n\n\n\n\n\n\n\n\n6. Read the documentation for geom_bar(). What does the weight aesthetic do?\nThe weight argument in the geom_bar() function allows you to adjust the heights of the bars according to the weights of the observations rather than simply counting the number of observations. This can be useful when you have a frequency or probability data set and you want to visualize it accurately.\ndata = tibble(\n  category = c(\"A\", \"A\", \"B\", \"B\", \"B\", \"C\", \"C\"),\n  weight = c(1, 2, 1, 3, 1, 2, 1)\n)\n\n# Plot without weights\nggplot(data, aes(category)) + geom_bar()\n# Plot with weights\nggplot(data, aes(category, weight = weight)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n7. Using the techniques already discussed in this chapter, come up with three ways to visualize a 2d categorical distribution. Try them out by visualizing the distribution of model and manufacturer, trans and class, and cyl and trans.\n\nggplot(mpg, aes(manufacturer, model)) + geom_point(aes(size = hwy))\n\n\n\n\n\n\nggplot(mpg, aes(class, fill = trans)) + geom_bar()\n\n\n\n\n\n\nggplot(mpg, aes(cyl)) + geom_bar() + facet_wrap(~trans, nrow = 2)\n\n\n\n\n\n\n\nNot the most insightful plots in the world, but the restrictions were met :)"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#modifying-the-axes",
    "href": "blog/ggplot2_book_3e_part1.html#modifying-the-axes",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.7 Modifying the Axes",
    "text": "2.7 Modifying the Axes\nThere are two families of useful helpers that let you make the most common modifications.\n\n\nxlab() and ylab() modify the x-axis and y-axis labels:\n\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3)\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) + \n  xlab(\"city driving (mpg)\") + ylab(\"highway driving (mpg)\")\n# Remove the axis labels with NULL\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) + xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxlim() and ylim() modify the limits of the axes:\n\nggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25)\nggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25) + xlim(\"f\", \"r\") + ylim(20, 30)\n# For continuous scales, use NA to set only one limit\nggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25, na.rm = TRUE) + ylim(NA, 30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nChanging the axes limits sets values outside the range to NA before it calculates summary statistics. You may use na.rm = TRUE to filter out the new NA values, but it is important to understand the order of operations."
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#output",
    "href": "blog/ggplot2_book_3e_part1.html#output",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.8 Output",
    "text": "2.8 Output\nPlots are usually generated to view immediately, but they can be saved to a variable:\n\np = ggplot(mpg, aes(displ, hwy, color = factor(cyl))) + geom_point()\n\nOnce you have a plot object, you can do a variety of things with it:\n\nRender it on screen with print() (this happens automatically when running interactively, but needs to be called explicitly in a loop or function):\n\n\nprint(p)\n\n\n\n\n\n\n\n\nSave it to disc with ggsave():\n\n\nggsave(\"plot.png\", p, width = 5 , height = 5)\n\n\nBriefly describe its structure with summary():\n\n\nsummary(p)\n\ndata: manufacturer, model, displ, year, cyl, trans, drv, cty, hwy, fl,\n  class [234x11]\nmapping:  x = ~displ, y = ~hwy, colour = ~factor(cyl)\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n-----------------------------------\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Science, Technology, & More",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMy R & RStudio Setup\n\n\n\n\n\n\nR\n\n\nRStudio\n\n\nWorkstation\n\n\nLiving Post\n\n\n\nThe code and the ‘why’ behind my setup, as well as some shoutouts.\n\n\n\n\n\nJun 13, 2024\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2: Elegant Graphics for Data Analysis (3e) | Part 2\n\n\nLayers\n\n\n\nR\n\n\n{ggplot2}\n\n\nTextbook Workthrough\n\n\n\nJoin me as I work through the exercises in the textbook.\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2: Elegant Graphics for Data Analysis (3e) | Part 1\n\n\nGetting Started\n\n\n\nR\n\n\n{ggplot2}\n\n\nTextbook Workthrough\n\n\n\nJoin me as I work through the exercises in the textbook.\n\n\n\n\n\nMay 24, 2024\n\n\n23 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ozan Ozbeker",
    "section": "",
    "text": "As a Data Scientist, I specialize in leveraging advanced analytics to solve complex business challenges, managing the entire data science process from importing and tidying data to modeling and communicating insights. With a strong foundation in R, I extensively use the Tidyverse and Tidymodels for data management and analysis, and I am skilled in importing data from databases, spreadsheets, web scraping, and API calls. My expertise includes creating automated ETL processes, developing complex SQL queries, and managing PostgreSQL databases. I have successfully led end-to-end client projects in the replacement parts business, building reporting tools and dashboards to support key decision-making for both B2B and B2C sales. I frequently use {ggplot2} for data visualization and deliver final insights through Tableau and other BI tools. My modeling experience includes forecasting, classification, and recommendation engines. Additionally, I am adept at building automated reports and executive dashboards using Quarto, incorporating HTML and CSS for customization. My ability to manage complete data pipelines and present findings to high-value stakeholders makes me a valuable asset in driving data-driven decision-making and strategic initiatives.\nOutside of work, I’m a huge fan of live music, especially Rock and EDM, and I’m making a bigger effort to travel and create experiences this year. I’ve been playing the guitar for about 10 years and creating a budget for around 3 years. When the weather’s nice, I enjoy cycling, hiking, and taking photos. Recently, I’ve developed a keen interest in learning more about the R language outside of my direct work, inspired by the fantastic blogs I’ve come across, which led me to create this website.\nOn this site, you’ll find my resume, personal projects, and a blog where I dive deep into various tools and technologies in the data world, along with other topics I find interesting. Feel free to reach out and connect!\n\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Summary",
    "section": "",
    "text": "As a data science consultant, I have managed end-to-end client projects in the replacement parts business, supporting key decision-making processes. This involved thorough requirements gathering, data integration from multiple sources, and the creation of insightful reports and dashboards to enhance decision-making and strategy formulation. Additionally, I automated internal workflows with SQL queries and formed cross-functional teams to assess and manage SKUs, ensuring effective inventory management through monthly reviews and automated reports.\nIn previous roles, I have focused extensively on Excess and Obsolete (E&O) inventory, developing automated workflows and comprehensive SQL queries to analyze and manage inventory. This includes automating the ETL process to align with company policies. My role has involved significant cross-departmental collaboration, working with finance, procurement, logistics, sustaining engineering, and product management to establish standardized workflows and create teams for monthly reviews of E&O inventory.\nI have extensive experience presenting to clients. This includes managing comprehensive data projects from requirements gathering to presenting results, conducting detailed client interviews, and delivering clear, actionable presentations. Internally, I have worked closely with various business functions to meet data pipeline and dashboard requirements, presenting findings to Director and VP-level stakeholders.\n\n\n\n\nR ProgrammingTableau, Tableau Prep, & Power BISQL & PythonExcel\n\n\nI am highly proficient in R within a professional setting, with a strong emphasis on the Tidyverse, Tidymodels, and related packages. My role as a “full-stack” data scientist involves managing the entire data science process framework, encompassing Import, Tidy, Transform, Visualize, Model, and Communicate stages.\n\n\n\nImport (& Data Management):\n\nData Sources: Extensive experience importing from databases, spreadsheets, web scraping, and APIs.\nDatabase Connections: Accessing client data through SQL Server, PostgreSQL, and SAP HANA.\nWeb Scraping and Automation: Creating and automating scrapes and API calls for data collection from services like Amazon and Walmart. Using Selenium to automate browsers and bypass bot detection.\nRecent Tools: Working with DuckDB for local data analysis and storage, though it is less frequently needed due to ample memory on work servers.\nData Management: Managing PostgreSQL databases for clients, including data modeling and management.\nKey Tools: {httr2}, {rvest}, {RSelenium}, {DBI}, DuckDB, Oxylabs, Helium 10\n\nTidy & Transform:\n\nData Wrangling: Extensive experience in focusing on grouping, filtering, and summarizing data.\nData Tidying: Transforming data by pivoting and converting data types (e.g., strings to dates, floats).\nRegex Expertise: Skilled in using regular expressions for data cleaning and manipulation.\nKey Tools: {tidyverse}, {janitor}\n\nVisualize:\n\nExploratory Data Analysis: Frequently use {ggplot2} and its extensions for data visualization.\nClient Preferences: While initial exploration and cleaning are done in R, clients prefer Tableau or other BI tools for final dashboards.\nKey Tools: {ggplot2}, {gganimate}, {ggthemes}\n\nModel:\n\nForecasting and Classification: Professional experience in forecasting with linear regression models and classification with support vector machines and recommendation engines for pricing.\nKey Tools: Base R Stats, {tidymodels}\n\nCommunicate:\n\nAutomated Reports: Building automated reports for clients to support financial decision-making, including dashboards and executive reports.\nCustomization: Using HTML and CSS to customize and enhance the visual appeal of reports. This website was built with Quarto and edited with custom CSS.\nKey Tools: Quarto, {gt}, {htmlwidgets}, {htmltools}\n\nProgramming:\n\nFunctional Programming: Using the {purrr} package for mapping functions and other utilities within the Tidyverse framework.\nVersion Control: Incorporating Git and GitHub into workflows and managing project structure.\nKey Tools: Task Scheduler, Git & GitHub, {fs}\n\n\n\n\n\n\n\nI have developed a strong competency in Tableau through both formal education and practical application in professional settings. Here is a detailed summary of my Tableau experience:\n\nLearning and Initial Skills Development:\n\nGained foundational knowledge of Tableau through DataCamp’s introductory course, which covered the most common functions and features.\nAcquired skills to create various charts and dashboards tailored to managerial needs.\n\nPractical Applications:\n\nDashboard Creation:\n\nCreated dashboards linked to the company’s ERP system, primarily displaying aggregated data for daily, weekly, and monthly performance metrics.\nDeveloped dashboards for clients that connect to their specific data sources, published on their Tableau Servers. These dashboards cover:\n\nParts distribution across distributors.\nPerformance metrics for Amazon marketplace.\nInternal supply chain performance monitoring.\n\nUtilized a mix of database connections, custom ETL processes, and multifaceted dashboards combining tables and charts for comprehensive stakeholder reporting.\n\nTableau Prep for ETL:\n\nUsed Tableau Prep for ETL tasks to prepare data for analysis and visualization in Tableau. Despite finding Tableau Prep to be slow, this experience motivated me to learn SQL for more efficient data processing.\n\n\nAdvanced Tableau Functions:\n\nMaintained multiple dashboards with regular updates and enhancements based on client feedback and evolving data needs.\nEnsured data integrity and accuracy through rigorous testing and validation of ETL processes and dashboard outputs.\n\n\n\n\n\nMy experience with Power BI stems from a company-wide transition from Tableau to Power BI, where I acquired the basics and applied them in a professional context. Here is a detailed summary of my Power BI experience:\n\nLearning and Initial Skills Development:\n\nGained foundational knowledge of Power BI during a transition from Tableau to Power BI as part of a Microsoft 365 (M365) integration at my previous company.\nLearned the basics of creating visualizations, reports, and dashboards within Power BI.\n\nPractical Applications:\n\nApplied Power BI skills to create and maintain reports and dashboards, similar to those previously developed in Tableau.\nUtilized Power BI’s features to connect to various data sources, perform data transformations, and build interactive visualizations.\n\n\n\n\n\n\n\nI have significant professional experience in creating complex SQL queries to generate custom reports for internal customers. This involves:\n\nComplex Queries:\n\nCommon Table Expressions (CTEs): Utilizing CTEs for better readability and modular query design.\nSubqueries: Embedding subqueries within main queries for dynamic data retrieval.\nNested Case-When Statements: Implementing conditional logic within queries to handle complex data manipulation.\nAggregate Calculations: Performing aggregations such as SUM, AVG, COUNT, etc., to summarize data.\nFiltering: Applying WHERE clauses and HAVING conditions to filter data as per requirements.\nDate/Time Functions: Manipulating and formatting date and time data.\nString Functions: Using functions like CONCAT, SUBSTRING, and LENGTH for string manipulation.\n\n\nThe most challenging aspects of reporting often involve not the SQL code itself but the process of identifying and validating the required data sources. Ensuring data accuracy and consistency is paramount in delivering reliable reports.\n\n\nIn my continuous pursuit of knowledge and skill enhancement, I have learned and practiced several advanced SQL techniques, including:\n\nWindow Functions: Applying functions like ROW_NUMBER, RANK, and LEAD/LAG to perform calculations across specific partitions of data.\nTransactions and Error Handling: Implementing transaction control (BEGIN, COMMIT, ROLLBACK) and error handling mechanisms to ensure data integrity.\nArrays: Using arrays for more complex data structures and operations.\nPivoting Tables: Creating pivot tables using the CROSSTAB function to transform data from rows to columns.\nCreating Totals and Grand Totals:\n\nROLLUP: Generating subtotals and grand totals in query results.\nCUBE: Extending ROLLUP to include cross-tabulated totals.\n\n\n\n\n\n\nPostgreSQL:\n\nMost of my SQL experience is in PostgreSQL, where I have developed and executed a wide range of queries and scripts.\n\nOther SQL Environments:\n\nMS Access: Occasionally used the SQL editor in MS Access for database management tasks.\nSQL Server: Completed a formal class in school focused on SQL Server, gaining foundational knowledge and skills in this environment.\n\n\n\n\n\n\n\nUndergraduate Teaching Assistant:\n\nWhile I haven’t used Python in a professional setting, my proficiency in Python was recognized when I served as an undergraduate teaching assistant. In this role, I:\n\nAssisted students in understanding and applying Python for data analysis and visualization.\nHelped design and grade assignments, projects, and exams.\nConducted review sessions and provided one-on-one tutoring to students.\n\nAlthough I haven’t used Python extensively in a professional setting recently, my foundational skills remain strong. I have a solid understanding of the core libraries and techniques needed for data manipulation, analysis, and visualization.\n\n\n\n\n\nPandas: Data manipulation, cleaning, aggregation, and merging.\nNumPy: Numerical computations, array manipulation, and mathematical functions.\nMatplotlib: Creating static visualizations, customizing plots, and adjusting aesthetics.\nSeaborn: Advanced statistical visualizations, integrating with Matplotlib, and enhancing visual appeal.\n\n\n\n\n\n\n\n\nBasic Functionalities:\n\nCreating tables and charts.\nImplementing conditional formatting.\nUsing nested IF statements for complex logical operations.\n\nAdvanced Functions:\n\nXLOOKUP: Efficiently searching for and retrieving data across large datasets.\nCOUNTIF: Counting cells that meet specific criteria.\nIFERROR: Handling errors gracefully in formulas.\nSUMPRODUCT: Performing array calculations for more complex data analysis.\nData Validation: Ensuring data integrity and consistency within spreadsheets.\n\n\n\n\n\nI am highly proficient in using Pivot Tables and Pivot Charts, which are essential for summarizing, analyzing, and visualizing data. These tools have been integral in my professional work for performing descriptive analytics, such as analyzing inventory levels, past statuses, and conducting ABC analyses to identify trends in usage.\n\n\n\nI frequently utilize Power Query for Extract, Transform, Load (ETL) functions, which allows me to automate data processing workflows. Power Query’s capabilities enable me to:\n\nExtract data from various sources.\nTransform and clean the data.\nLoad the processed data into Excel for analysis.\n\nThis automation significantly enhances efficiency and accuracy in data handling.\n\n\n\nIn a professional setting, I have applied my Excel skills primarily for:\n\nDescriptive analytics related to inventory management.\nConducting ABC analyses to classify inventory based on importance.\nIdentifying trends and patterns in data usage.\n\n\n\n\nIn school, I designed an ERP system within Excel for a small business simulation. This project involved:\n\nCreating Bills of Materials (BOM) for two products.\nForecasting sales using dummy data.\nCalculating safety stock levels.\nSetting product prices based on estimated labor and material costs.\n\n\n\nWhile I have dabbled in VBA for creating macros, I prefer using standard Excel functions for most tasks to ensure that my tools are user-friendly and easily accessible to others. My experience with working and sharing files in Excel within Teams (formerly known as SharePoint) has further reduced the need for macros, as the collaborative environment supports most of my ETL requirements through Power Query."
  },
  {
    "objectID": "resume.html#data-analytics-consulting",
    "href": "resume.html#data-analytics-consulting",
    "title": "Summary",
    "section": "",
    "text": "As a data science consultant, I have managed end-to-end client projects in the replacement parts business, supporting key decision-making processes. This involved thorough requirements gathering, data integration from multiple sources, and the creation of insightful reports and dashboards to enhance decision-making and strategy formulation. Additionally, I automated internal workflows with SQL queries and formed cross-functional teams to assess and manage SKUs, ensuring effective inventory management through monthly reviews and automated reports.\nIn previous roles, I have focused extensively on Excess and Obsolete (E&O) inventory, developing automated workflows and comprehensive SQL queries to analyze and manage inventory. This includes automating the ETL process to align with company policies. My role has involved significant cross-departmental collaboration, working with finance, procurement, logistics, sustaining engineering, and product management to establish standardized workflows and create teams for monthly reviews of E&O inventory.\nI have extensive experience presenting to clients. This includes managing comprehensive data projects from requirements gathering to presenting results, conducting detailed client interviews, and delivering clear, actionable presentations. Internally, I have worked closely with various business functions to meet data pipeline and dashboard requirements, presenting findings to Director and VP-level stakeholders."
  },
  {
    "objectID": "resume.html#core-competencies",
    "href": "resume.html#core-competencies",
    "title": "Summary",
    "section": "",
    "text": "R ProgrammingTableau, Tableau Prep, & Power BISQL & PythonExcel\n\n\nI am highly proficient in R within a professional setting, with a strong emphasis on the Tidyverse, Tidymodels, and related packages. My role as a “full-stack” data scientist involves managing the entire data science process framework, encompassing Import, Tidy, Transform, Visualize, Model, and Communicate stages.\n\n\n\nImport (& Data Management):\n\nData Sources: Extensive experience importing from databases, spreadsheets, web scraping, and APIs.\nDatabase Connections: Accessing client data through SQL Server, PostgreSQL, and SAP HANA.\nWeb Scraping and Automation: Creating and automating scrapes and API calls for data collection from services like Amazon and Walmart. Using Selenium to automate browsers and bypass bot detection.\nRecent Tools: Working with DuckDB for local data analysis and storage, though it is less frequently needed due to ample memory on work servers.\nData Management: Managing PostgreSQL databases for clients, including data modeling and management.\nKey Tools: {httr2}, {rvest}, {RSelenium}, {DBI}, DuckDB, Oxylabs, Helium 10\n\nTidy & Transform:\n\nData Wrangling: Extensive experience in focusing on grouping, filtering, and summarizing data.\nData Tidying: Transforming data by pivoting and converting data types (e.g., strings to dates, floats).\nRegex Expertise: Skilled in using regular expressions for data cleaning and manipulation.\nKey Tools: {tidyverse}, {janitor}\n\nVisualize:\n\nExploratory Data Analysis: Frequently use {ggplot2} and its extensions for data visualization.\nClient Preferences: While initial exploration and cleaning are done in R, clients prefer Tableau or other BI tools for final dashboards.\nKey Tools: {ggplot2}, {gganimate}, {ggthemes}\n\nModel:\n\nForecasting and Classification: Professional experience in forecasting with linear regression models and classification with support vector machines and recommendation engines for pricing.\nKey Tools: Base R Stats, {tidymodels}\n\nCommunicate:\n\nAutomated Reports: Building automated reports for clients to support financial decision-making, including dashboards and executive reports.\nCustomization: Using HTML and CSS to customize and enhance the visual appeal of reports. This website was built with Quarto and edited with custom CSS.\nKey Tools: Quarto, {gt}, {htmlwidgets}, {htmltools}\n\nProgramming:\n\nFunctional Programming: Using the {purrr} package for mapping functions and other utilities within the Tidyverse framework.\nVersion Control: Incorporating Git and GitHub into workflows and managing project structure.\nKey Tools: Task Scheduler, Git & GitHub, {fs}\n\n\n\n\n\n\n\nI have developed a strong competency in Tableau through both formal education and practical application in professional settings. Here is a detailed summary of my Tableau experience:\n\nLearning and Initial Skills Development:\n\nGained foundational knowledge of Tableau through DataCamp’s introductory course, which covered the most common functions and features.\nAcquired skills to create various charts and dashboards tailored to managerial needs.\n\nPractical Applications:\n\nDashboard Creation:\n\nCreated dashboards linked to the company’s ERP system, primarily displaying aggregated data for daily, weekly, and monthly performance metrics.\nDeveloped dashboards for clients that connect to their specific data sources, published on their Tableau Servers. These dashboards cover:\n\nParts distribution across distributors.\nPerformance metrics for Amazon marketplace.\nInternal supply chain performance monitoring.\n\nUtilized a mix of database connections, custom ETL processes, and multifaceted dashboards combining tables and charts for comprehensive stakeholder reporting.\n\nTableau Prep for ETL:\n\nUsed Tableau Prep for ETL tasks to prepare data for analysis and visualization in Tableau. Despite finding Tableau Prep to be slow, this experience motivated me to learn SQL for more efficient data processing.\n\n\nAdvanced Tableau Functions:\n\nMaintained multiple dashboards with regular updates and enhancements based on client feedback and evolving data needs.\nEnsured data integrity and accuracy through rigorous testing and validation of ETL processes and dashboard outputs.\n\n\n\n\n\nMy experience with Power BI stems from a company-wide transition from Tableau to Power BI, where I acquired the basics and applied them in a professional context. Here is a detailed summary of my Power BI experience:\n\nLearning and Initial Skills Development:\n\nGained foundational knowledge of Power BI during a transition from Tableau to Power BI as part of a Microsoft 365 (M365) integration at my previous company.\nLearned the basics of creating visualizations, reports, and dashboards within Power BI.\n\nPractical Applications:\n\nApplied Power BI skills to create and maintain reports and dashboards, similar to those previously developed in Tableau.\nUtilized Power BI’s features to connect to various data sources, perform data transformations, and build interactive visualizations.\n\n\n\n\n\n\n\nI have significant professional experience in creating complex SQL queries to generate custom reports for internal customers. This involves:\n\nComplex Queries:\n\nCommon Table Expressions (CTEs): Utilizing CTEs for better readability and modular query design.\nSubqueries: Embedding subqueries within main queries for dynamic data retrieval.\nNested Case-When Statements: Implementing conditional logic within queries to handle complex data manipulation.\nAggregate Calculations: Performing aggregations such as SUM, AVG, COUNT, etc., to summarize data.\nFiltering: Applying WHERE clauses and HAVING conditions to filter data as per requirements.\nDate/Time Functions: Manipulating and formatting date and time data.\nString Functions: Using functions like CONCAT, SUBSTRING, and LENGTH for string manipulation.\n\n\nThe most challenging aspects of reporting often involve not the SQL code itself but the process of identifying and validating the required data sources. Ensuring data accuracy and consistency is paramount in delivering reliable reports.\n\n\nIn my continuous pursuit of knowledge and skill enhancement, I have learned and practiced several advanced SQL techniques, including:\n\nWindow Functions: Applying functions like ROW_NUMBER, RANK, and LEAD/LAG to perform calculations across specific partitions of data.\nTransactions and Error Handling: Implementing transaction control (BEGIN, COMMIT, ROLLBACK) and error handling mechanisms to ensure data integrity.\nArrays: Using arrays for more complex data structures and operations.\nPivoting Tables: Creating pivot tables using the CROSSTAB function to transform data from rows to columns.\nCreating Totals and Grand Totals:\n\nROLLUP: Generating subtotals and grand totals in query results.\nCUBE: Extending ROLLUP to include cross-tabulated totals.\n\n\n\n\n\n\nPostgreSQL:\n\nMost of my SQL experience is in PostgreSQL, where I have developed and executed a wide range of queries and scripts.\n\nOther SQL Environments:\n\nMS Access: Occasionally used the SQL editor in MS Access for database management tasks.\nSQL Server: Completed a formal class in school focused on SQL Server, gaining foundational knowledge and skills in this environment.\n\n\n\n\n\n\n\nUndergraduate Teaching Assistant:\n\nWhile I haven’t used Python in a professional setting, my proficiency in Python was recognized when I served as an undergraduate teaching assistant. In this role, I:\n\nAssisted students in understanding and applying Python for data analysis and visualization.\nHelped design and grade assignments, projects, and exams.\nConducted review sessions and provided one-on-one tutoring to students.\n\nAlthough I haven’t used Python extensively in a professional setting recently, my foundational skills remain strong. I have a solid understanding of the core libraries and techniques needed for data manipulation, analysis, and visualization.\n\n\n\n\n\nPandas: Data manipulation, cleaning, aggregation, and merging.\nNumPy: Numerical computations, array manipulation, and mathematical functions.\nMatplotlib: Creating static visualizations, customizing plots, and adjusting aesthetics.\nSeaborn: Advanced statistical visualizations, integrating with Matplotlib, and enhancing visual appeal.\n\n\n\n\n\n\n\n\nBasic Functionalities:\n\nCreating tables and charts.\nImplementing conditional formatting.\nUsing nested IF statements for complex logical operations.\n\nAdvanced Functions:\n\nXLOOKUP: Efficiently searching for and retrieving data across large datasets.\nCOUNTIF: Counting cells that meet specific criteria.\nIFERROR: Handling errors gracefully in formulas.\nSUMPRODUCT: Performing array calculations for more complex data analysis.\nData Validation: Ensuring data integrity and consistency within spreadsheets.\n\n\n\n\n\nI am highly proficient in using Pivot Tables and Pivot Charts, which are essential for summarizing, analyzing, and visualizing data. These tools have been integral in my professional work for performing descriptive analytics, such as analyzing inventory levels, past statuses, and conducting ABC analyses to identify trends in usage.\n\n\n\nI frequently utilize Power Query for Extract, Transform, Load (ETL) functions, which allows me to automate data processing workflows. Power Query’s capabilities enable me to:\n\nExtract data from various sources.\nTransform and clean the data.\nLoad the processed data into Excel for analysis.\n\nThis automation significantly enhances efficiency and accuracy in data handling.\n\n\n\nIn a professional setting, I have applied my Excel skills primarily for:\n\nDescriptive analytics related to inventory management.\nConducting ABC analyses to classify inventory based on importance.\nIdentifying trends and patterns in data usage.\n\n\n\n\nIn school, I designed an ERP system within Excel for a small business simulation. This project involved:\n\nCreating Bills of Materials (BOM) for two products.\nForecasting sales using dummy data.\nCalculating safety stock levels.\nSetting product prices based on estimated labor and material costs.\n\n\n\nWhile I have dabbled in VBA for creating macros, I prefer using standard Excel functions for most tasks to ensure that my tools are user-friendly and easily accessible to others. My experience with working and sharing files in Excel within Teams (formerly known as SharePoint) has further reduced the need for macros, as the collaborative environment supports most of my ETL requirements through Power Query."
  },
  {
    "objectID": "resume.html#data-scientist",
    "href": "resume.html#data-scientist",
    "title": "Summary",
    "section": "Data Scientist",
    "text": "Data Scientist\nRXA @ OneMagnify - Remote, USA | January 2023 - Current\n\nClient Work: Led end-to-end data science projects, developed dashboards, and managed client relationships. Developed a Tableau Dashboard for a major client to monitor Parts Distributors’ (PD) performance on Amazon, integrating sales, inventory, return, and review data to assess PD performance, strengthen relationships, and identify top-performing parts for promotions. Implemented a monthly Amazon marketplace scrape to track knock-off parts listings and estimate sales and revenue, with data aggregated in the Tableau Dashboard to highlight competitive market dynamics. Created a weekly price recommendation report for another client, incorporating knock-off data, confidential price sheets, margin information, and competitive ASIN analysis using R and Quarto. Maintained continuous communication with clients through biweekly meetings and regular email updates.\nInternal Projects: Extracting and analyzing data, developing automated reports, and managing databases. Employed various data extraction methods, including connecting to MS SQL Server and SAP HANA via VPN, and ingesting CSV, TSV, and XLSX files into PostgreSQL using R and {tidyverse} packages. Utilized tools like Oxylabs, Helium10, and {httr2} for web scraping and modeling competitive Amazon marketplace dynamics. Streamlined workflows using R for data manipulation, saving data in flat files for compatibility with Tableau. Developed automated email reports using R and Quarto, integrating data from PostgreSQL and rendering with {gt} and {ggplot2}. Managed PostgreSQL databases by applying database schema knowledge and tidy data principles. Implemented structured version control with Git/GitHub, organizing project directories according to CRAN standards."
  },
  {
    "objectID": "resume.html#continuous-improvement-specialist-oldp-rotation-2",
    "href": "resume.html#continuous-improvement-specialist-oldp-rotation-2",
    "title": "Summary",
    "section": "Continuous Improvement Specialist (OLDP Rotation 2)",
    "text": "Continuous Improvement Specialist (OLDP Rotation 2)\nXylem, Inc - Chicago, IL | July 2022 - December 2022\n\nExcess & Obsolete Inventory Identification & Disposition: Employed the DMAIC/DMADV methodology to create a process aimed at reducing the site’s E&O financial reserve and material in warehouses by identifying obsolete items and creating opportunities for rework, resale, and disposal. The control plan included establishing a cross-departmental E&O taskforce, with the new process projected to achieve estimated hard savings of $1M - $4M over the next 3-5 years. Utilized skills in Process Flowcharts, Value Stream Mapping, and Control Charts.\nAd-Hoc Floor Workshops: Led and assisted various projects to drive Continuous Improvement initiatives at the site. These included 5S audits, lean methodology seminars, inventory audits, and cycle counts, applying skills in 5S, FMEA, and Process Capability Analysis."
  },
  {
    "objectID": "resume.html#business-data-analyst-oldp-rotation-1",
    "href": "resume.html#business-data-analyst-oldp-rotation-1",
    "title": "Summary",
    "section": "Business Data Analyst (OLDP Rotation 1)",
    "text": "Business Data Analyst (OLDP Rotation 1)\nXylem, Inc - Uniontown, PA | July 2021 - July 2022\n\nExcess & Obsolete Reserve: Designed a recommendation engine for the quarterly report, focusing on reducing ETL time and standardizing the process across all Sensus NA sites to comply with new financial policies. This effort reduced process time by 72 hours per month and enabled new reporting and forecasting capabilities. Leveraged Advanced Excel (Power Query, PivotTables, XLOOKUP) and Microsoft SQL Server.\nSensus North America Physical Inventory: Created documentation for the annual NA physical inventory event, including process flowcharts, work instructions, and manager checklists for tasks leading up to the event. This documentation package standardized the process across all NA sites and eliminated non-value-added activities during the event, utilizing skills in Project Management and Process Flowcharts.\nAd-Hoc Reporting: Developed various data pipelines, dashboards, and reports to meet unique business needs. This included tracking forecasting accuracy, comparing supplier and internal bill of materials, gathering and visualizing quality control measures, and synchronizing lot sizes across multiple sites."
  },
  {
    "objectID": "resume.html#undergraduate-teaching-assistant",
    "href": "resume.html#undergraduate-teaching-assistant",
    "title": "Summary",
    "section": "Undergraduate Teaching Assistant",
    "text": "Undergraduate Teaching Assistant\nWest Virginia University - Morgantown, WV | August 2019 - May 2021\n\nSpring 2021 | IENG 220: Re-engineering Management Systems\nFall 2020 | IENG 305: Introduction to Systems Engineering\nSpring 2020 | IENG 331: Computer Applications in Industrial Engineering\nFall 2019 | IENG 445: Project Management for Engineers"
  },
  {
    "objectID": "resume.html#manufacturing-engineer-intern",
    "href": "resume.html#manufacturing-engineer-intern",
    "title": "Summary",
    "section": "Manufacturing Engineer Intern",
    "text": "Manufacturing Engineer Intern\nJLG Industries - McConnellsburg, PA | June 2019 - August 2019\n\nLong-term Project: Test – Inspect – Green Tag (TIG) Line\n\nConducted a full 5S sweep of the line, removing unnecessary equipment and parts from work stations and laid corners.\nCreated from scratch Standard Work Instructions (SWI) for all 5 stations of the TIG Line for 3 different products.\nImplemented a scanner-based system for entering defects for Electronic Quality Control (EQC), standardizing the process and reducing variability in defect types and improving data collection.\n\nShort-term Project: Tire Manipulator\n\nCreated floor plans for current and future station layouts for the new Tire Manipulator with laser measures and AutoCAD."
  },
  {
    "objectID": "resume.html#bachelor-of-science-in-industrial-engineering",
    "href": "resume.html#bachelor-of-science-in-industrial-engineering",
    "title": "Summary",
    "section": "Bachelor of Science in Industrial Engineering",
    "text": "Bachelor of Science in Industrial Engineering\nWest Virginia University | Morgantown, WV"
  },
  {
    "objectID": "resume.html#certifications",
    "href": "resume.html#certifications",
    "title": "Summary",
    "section": "Certifications",
    "text": "Certifications\n\nLean Six Sigma Green Belt | Institute of Industrial and Systems Engineers\nContinuous Improvement Fundamentals | Oshkosh Corporation\nEligible for Certified Associate in Project Management (CAPM) | Project Management Institute"
  },
  {
    "objectID": "blog/r-rstudio-setup.html",
    "href": "blog/r-rstudio-setup.html",
    "title": "My R & RStudio Setup",
    "section": "",
    "text": "This is a hybrid blog post/README/code file where it is the actual code file I use when I update my R environment/computer, but also I wrote some descriptions along the way so if somebody else stumbles upon this and likes what they see, they can copy it or maybe even find something new that they didn’t know about before. The link to my GitHub repo is on the right panel. Feel free to fork and customize it for your use!\n\n\n\n\n\n\nNote\n\n\n\nThis is what I’m going to call a “Living Post”, where it’s living in the sense that I plan on updating it over time, and just republish instead of creating a new one."
  },
  {
    "objectID": "blog/r-rstudio-setup.html#packages",
    "href": "blog/r-rstudio-setup.html#packages",
    "title": "My R & RStudio Setup",
    "section": "\n2.1 Packages",
    "text": "2.1 Packages\nThese are the packages I most frequently use, loosely grouped into the categories below. This does not include dependency packages. So for example, I use quarto to render Quarto documents via R code, but I don’t directly use the markdown package myself, so it is not listed below.\nYou can find the info page for each package at https://cran.r-project.org/web/packages/[package_name]/index.html where [package_name] is the name of the package.\n\n\n\nr-and-rstudio-setup.R\n\ninstall.packages(c(\n  # Meta Packages\n  'tidyverse',  # Easily Install and Load the \"Tidyverse\" packages | https://tidyverse.tidyverse.org\n  'tidymodels', # Easily Install and Load the \"Tidymodels\" packages | https://tidymodels.tidymodels.org/\n  \n  # Programming & Development\n  'purrr',    # Functional Programming Tools | https://purrr.tidyverse.org/\n  'forcats',  # Tools for Working with Categorical Variables (Factors) | https://forcats.tidyverse.org/\n  'keyring',  # Package for accessing OS's credential store | https://keyring.r-lib.org/\n  'fs',       # Cross-Platform File System Operations Based on 'libuv' | https://fs.r-lib.org/\n  'renv',     # Project Environments | https://rstudio.github.io/renv/index.html\n\n  # Import\n  'readr',    # Read Rectangular Data | https://readr.tidyverse.org/\n  'httr2',    # Perform HTTP Requests and Process the Responses | https://httr2.r-lib.org/\n  'readxl',   # Read Excel Files | https://readxl.tidyverse.org/\n  'rvest',    # Easily Harvest (Scrape) Web Pages | https://rvest.tidyverse.org/\n  'jsonlite', # A Simple and Robust JSON Parser and Generator for R | https://jeroen.r-universe.dev/jsonlite\n  \n  # Tidy/Transform\n  'dplyr',     # A Grammar of Data Manipulation | https://dplyr.tidyverse.org/\n  'tidyr',     # Tidy Messy Data | https://tidyr.tidyverse.org/\n  'tibble',    # Simple Data Frames | https://tibble.tidyverse.org/\n  'stringr',   # Simple, Consistent Wrappers for Common String Operations | https://stringr.tidyverse.org/\n  'lubridate', # Make Dealing with Dates a Little Easier | https://lubridate.tidyverse.org/\n  'janitor',   # Simple Tools for Examining and Cleaning Dirty Data | https://sfirke.github.io/janitor/index.html \n\n  # Visualize\n  'ggplot2',   # Create Elegant Data Visualizations Using the Grammar of Graphics | https://ggplot2.tidyverse.org/\n  'gt',        # Easily Create Presentation-Ready Display Tables | https://gt.rstudio.com/\n  'skimr',     # Compart and Flexible Summaries of Data | https://docs.ropensci.org/skimr/\n  \n  # Model\n  'rsample',   # General Re-sampling Infrastructure | https://rsample.tidymodels.org/\n  'parsnip',   # A Common API to Modeling and Analysis Functions | https://parsnip.tidymodels.org/\n  'recipes',   # Pre-processing and Feature Engineering Steps for Modeling | https://recipes.tidymodels.org/\n  'workflows', # Modeling Workflows | https://workflows.tidymodels.org/\n  'tune',      # Tidy Tuning Tools | https://tune.tidymodels.org/\n  'yardstick', # Tidy Characterizations of Model Performance | https://yardstick.tidymodels.org/\n  'broom',     # Convert Statistical Objects into Tidy Tibbles | https://broom.tidymodels.org/\n  'dials',     # Tools for Creating Tuning Parameter Values | https://dials.tidymodels.org/\n  'infer',     # Tidy Statistical Inference | https://infer.tidymodels.org/\n  'corrr',     # Correlations in R | https://corrr.tidymodels.org/\n  \n  # Communicate\n  'quarto', # R Interface to 'Quarto' Markdown Publishing System\n\n  # Database\n  'DBI',     # R Database Interface | https://dbi.r-dbi.org/index.html\n  'odbc',    # Connect to ODBC Compatible Databases (using the {DBI} Interface) | https://odbc.r-dbi.org/\n  'dbplyr',  # A {dplyr} Back End for Databases | https://dbplyr.tidyverse.org/\n  'duckdb',  # {DBI} Package for the DuckDB Database Management System | https://r.duckdb.org/\n  'duckplyr' # A DuckDB-backed Version of {dplyr} | https://duckdblabs.github.io/duckplyr/\n))\n\ninstall.packages(\n  \"rsthemes\",\n  repos = c(gadenbuie = 'https://gadenbuie.r-universe.dev', getOption(\"repos\"))\n)\n\nrsthemes::install_rsthemes()\n\n# Other packages I'm interested in:\n# 'profvis'  # Interactive Visualizations for Profiling R Code | https://rstudio.github.io/profvis/\n# 'targets'  # Pipelining Tools in R | https://docs.ropensci.org/targets/\n# 'testthat' # Unit Testing for R | https://testthat.r-lib.org\n# 'usethis'  # Automate Package and Project Setup | https://usethis.r-lib.org/\n# 'devtools' # Tools to Make Developing R Packages Easier | https://devtools.r-lib.org/\n# 'zoo'      # S3 Infrastructure for Regular and Irregular Time Series (Z's Ordered Observations) | https://zoo.R-Forge.R-project.org/\n\n\nAfter these are installed, close and open RStudio again so that the addins from the packages also load."
  },
  {
    "objectID": "blog/r-rstudio-setup.html#rstudio-configuration",
    "href": "blog/r-rstudio-setup.html#rstudio-configuration",
    "title": "My R & RStudio Setup",
    "section": "\n2.2 RStudio Configuration",
    "text": "2.2 RStudio Configuration\nAs I’ve been using R & RStudio, I’ve learned which custom settings I like the most. This section of code takes the respective files and copies them to where the base files for R and RStudio are. I created this repo because of my job where we log into different virtual machines. I got sick of resetting all of these options by hand, so now I just pull this repo from GitHub and run the code.\nThe directories don’t exist by default, they are created when you manually change the respective settings, so this chunk takes care of that, especially if it’s a fresh install of RStudio.\n\n\n\nr-and-rstudio-setup.R\n\npurrr::walk(\n  .x = c('themes', 'keybindings', 'snippets'),\n  .f = \\(directory) stringr::str_c(\"C:/Users/\", {Sys.info()[['user']]}, \"/AppData/Roaming/RStudio/\", directory) |&gt; fs::dir_create()\n)\n\n\nThen we copy the files from this folder to their appropriate locations.\n\n\n\nr-and-rstudio-setup.R\n\npurrr::pwalk(\n  .l = tibble::tribble(\n    ~file,                   ~destination,\n    'rstudio_bindings.json',  stringr::str_glue(\"C:/Users/{Sys.info()[['user']]}/AppData/Roaming/RStudio/keybindings/rstudio_bindings.json\"),\n    'addins.json',            stringr::str_glue(\"C:/Users/{Sys.info()[['user']]}/AppData/Roaming/RStudio/keybindings/addins.json\"),\n    'r.snippets',             stringr::str_glue(\"C:/Users/{Sys.info()[['user']]}/AppData/Roaming/RStudio/snippets/r.snippets\"),\n    'rstudio-prefs.json',     stringr::str_glue(\"C:/Users/{Sys.info()[['user']]}/AppData/Roaming/RStudio/rstudio-prefs.json\"),\n    '.Rprofile',              stringr::str_glue(\"C:/Users/{Sys.info()[['user']]}/Documents/.Rprofile\")),\n  .f = \\(file, destination) fs::file_copy(path = file, new_path = destination, overwrite = TRUE)\n)"
  },
  {
    "objectID": "blog/r-rstudio-setup.html#r-themes",
    "href": "blog/r-rstudio-setup.html#r-themes",
    "title": "My R & RStudio Setup",
    "section": "\n2.3 R Themes",
    "text": "2.3 R Themes\nI hate that when you use dark mode in RStudio, it’s just this dark blue shade for the UI, not actually dark/black. Also, I’m really not a fan of any of the themes that come with RStudio. To some extent, I think Garrick Aden-Buie also agreed with my sentiment and created this awesome package, {rsthemes}.\n\n2.3.1 {rthemes}\nThis package comes with a bunch of awesome themes that complete change the way RStudio looks and feels, and I would recommend it to anybody looking to modernize their RStudio interface. You can see the usage in the docs, and the settings I’m using in my .Rprofile, which I talk about below.\nRight now, I really like the Elm light and Elm dark themes, and I switch between the two depending on the brightness of the room.\n\n2.3.2 {darkstudio}\nI’m now using the {rsthemes} package, but I have to give a huge shoutout to GitHub user rileytwo. I used rileytwo’s work below for more than a year, and I can’t not shout it out.\n\n\n{darkstudio} is a package that turns the IDE elements to shades of black whenever you are using a theme with rs-theme-is-dark: TRUE.\n\nKiss: Keep It Stupid Simple is a theme from Riley that just looks so good, it’s been my go to for over a year now.\n\nIf either of these interest you, please go check them out. They were a staple to my R experience and I would like to support the creator as best as I can.\n\ndevtools::install_github('rileytwo/darkstudio')\ndarkstudio::activate() # This requires admin privileges"
  },
  {
    "objectID": "blog/r-rstudio-setup.html#rprofile",
    "href": "blog/r-rstudio-setup.html#rprofile",
    "title": "My R & RStudio Setup",
    "section": "\n3.1 .Rprofile",
    "text": "3.1 .Rprofile\nThe .Rprofile file is an R script that runs each time R starts up that lets you to customize your R environment. It can be used to set global options, load packages, define functions, and customize the R startup process. Or in my case, have some fun 😁\n\n3.1.1 Options\noptions() lets you set the global options for R. All of them have a default, and these are the ones I’ve changed for my preference. You can use ?options to view the full list.\n\n\n\n.Rprofile\n\noptions(\n  scipen = 999999,               # A penalty to be applied to turning regular values to scientific notation\n  setWidthOnResize = TRUE,       # If set and TRUE, the terminal resize when the terminal pane is resized\n  useFancyQuotes = FALSE,        # Turn off fancy quotes\n  warn = 1,                      # Prints warnings as they happen instead of when the top function ends.\n  warnPartialMatchArgs = TRUE,   # Warns if partial matching is used in argument matching.\n  warnPartialMatchAttr = TRUE,   # Warns if partial matching is used in extracting attributes via attr.\n  warnPartialMatchDollar = TRUE, # Warns if partial matching is used for extraction by $.\n  prompt = 'R&gt; ',                # This is complete personal preference\n  continue = '+&gt; ',              # This is complete personal preference\n  rsthemes.theme_light = \"Elm light {rsthemes}\",\n  rsthemes.theme_dark = \"Elm dark {rsthemes}\",\n  rsthemes.theme_favorite = \"Elm dark {rsthemes}\"\n)\n\n\n\n3.1.2 R Completion Options\nThe rc.settings function is part of the utils package in R, which configures settings for the R completion mechanism. It can enable or disable specific completion features like inter-package completion, function argument completion, and fuzzy matching. You can use ?rc.settings to view the full list.\n\n\nipck (Inter-Package Completion):\n\nDescription: This option enables or disables completion of object names across all loaded packages.\nExplanation: If you work with multiple packages, you can quickly access functions and objects from any loaded package without having to remember which package they belong to.\n\n\n\nfunc (Function Argument Completion):\n\nDescription: This option enables or disables completion of function argument names.\nExplanation: This helps you to write function calls more efficiently by providing argument suggestions, reducing the need to look up function documentation frequently.\n\n\n\nfuzzy (Fuzzy Matching):\n\nDescription: This option enables or disables fuzzy matching for completion.\nExplanation: This is useful when you cannot recall the exact name of a function or object. Fuzzy matching will suggest completions that closely resemble what you have typed, saving time and reducing frustration.\n\n\n\nIn short, the more help I can get coding, the better 😁\n\n\n\n.Rprofile\n\nrc.settings(ipck = TRUE, func = TRUE, fuzzy = TRUE)\n\n\n\n3.1.3 Interactive Sessions\nYou can use this code section if you want things to enable in an interactive session, like auto-loading certain packages. Some people use it to load packages that they always use, like tidyverse, usethis, devtools, and so on, but I’m in the camp that every script should be very explicit in what packages are being used, so I use this feature to have some fun.\nIf in an interactive session, this code will output my message in the console everytime R starts. I’ve always wanted my own J.A.R.V.I.S., R is close enough 🥲\n\n\n\n.Rprofile\n\nif (interactive()) {\n  message(\"Welcome Mr. Ozbeker, how may I be of assistance?\")\n}"
  },
  {
    "objectID": "blog/r-rstudio-setup.html#r.snippets",
    "href": "blog/r-rstudio-setup.html#r.snippets",
    "title": "My R & RStudio Setup",
    "section": "\n3.2 r.snippets",
    "text": "3.2 r.snippets\nRStudio lets you add custom snippets (pieces of code that can be quickly inserted, useful for repetitive code) and key bindings that can really help your code flow if you use them.\n\n\n\nr.snippets\n\nsnippet user\n  user = Sys.info()[['user']]"
  },
  {
    "objectID": "blog/r-rstudio-setup.html#rstudio_bindings.json",
    "href": "blog/r-rstudio-setup.html#rstudio_bindings.json",
    "title": "My R & RStudio Setup",
    "section": "\n3.3 rstudio_bindings.json",
    "text": "3.3 rstudio_bindings.json\nThis may seem like a simple change, but I though it was weird that you have to press 3 keys for arguably the most important operator in R. Also mapping to the actual pipe key, |, made more sense to me. (Well yes it’s technically mapping to \\ but having to hit shift would’ve removed half the reason of remapping in the first place.)\n\n\n\nrstudio_bindings.json\n\n{\n    \"insertPipeOperator\": \"Ctrl+\\\\\"\n}"
  },
  {
    "objectID": "blog/r-rstudio-setup.html#addins.json",
    "href": "blog/r-rstudio-setup.html#addins.json",
    "title": "My R & RStudio Setup",
    "section": "\n3.4 addins.json",
    "text": "3.4 addins.json\nRStudio uses a separate file for key bindings for addins, and the {rsthemes} package comes with a feature for toggling from light and dark modes, and a recommended key binding to follow:\n\n\n\naddins.json\n\n{\n    \"rsthemes::use_theme_toggle\": \"Ctrl+Alt+D\"\n}"
  },
  {
    "objectID": "blog/r-rstudio-setup.html#rstudio-prefs.json",
    "href": "blog/r-rstudio-setup.html#rstudio-prefs.json",
    "title": "My R & RStudio Setup",
    "section": "\n3.5 rstudio-prefs.json",
    "text": "3.5 rstudio-prefs.json\nLast but not least, we have the file that began it all. rstudio-prefs.json holds all of your custom settings that you change in Tools &gt; Global Options. Every time I had to setup RStudio on a new machine, I could never remember all the settings I liked, and I figured that there was a file somewhere which holds these settings that I could overwrite. That is exactly what this file is. These settings allow you to customize various aspects of the IDE, such as appearance, editor behavior, and default options for projects.\nJSON files don’t have a great way of adding in-line comments, but the settings names are pretty descriptive of what they do. Since this is the last section of the blog post, I have no shame printing all 67 lines 😈\n\n\n\nrstudio-prefs.json\n\n{\n    \"restore_source_documents\": false,\n    \"wrap_tab_navigation\": false,\n    \"save_workspace\": \"never\",\n    \"load_workspace\": false,\n    \"initial_working_directory\": \"C:/Users/OzanO/Code\",\n    \"default_open_project_location\": \"C:/Users/OzanO/Code\",\n    \"always_save_history\": false,\n    \"restore_last_project\": false,\n    \"auto_detect_indentation\": true,\n    \"insert_native_pipe_operator\": true,\n    \"highlight_selected_line\": true,\n    \"scroll_past_end_of_document\": true,\n    \"highlight_r_function_calls\": true,\n    \"rainbow_parentheses\": true,\n    \"auto_append_newline\": true,\n    \"strip_trailing_whitespace\": true,\n    \"warn_if_no_such_variable_in_scope\": true,\n    \"warn_variable_defined_but_not_used\": true,\n    \"style_diagnostics\": true,\n    \"show_diagnostics_other\": true,\n    \"indent_guides\": \"gray\",\n    \"syntax_color_console\": true,\n    \"limit_visible_console\": true,\n    \"auto_expand_error_tracebacks\": true,\n    \"rmd_chunk_output_inline\": false,\n    \"show_rmd_render_command\": true,\n    \"rmd_viewer_type\": \"pane\",\n    \"pdf_previewer\": \"none\",\n    \"windows_terminal_shell\": \"win-ps\",\n    \"full_project_path_in_window_title\": true,\n    \"panes\": {\n        \"quadrants\": [\n            \"Source\",\n            \"TabSet1\",\n            \"Console\",\n            \"TabSet2\"\n        ],\n        \"tabSet1\": [\n            \"Environment\",\n            \"History\",\n            \"Connections\",\n            \"Build\",\n            \"VCS\",\n            \"Presentation\"\n        ],\n        \"tabSet2\": [\n            \"Files\",\n            \"Plots\",\n            \"Packages\",\n            \"Help\",\n            \"Viewer\",\n            \"Presentations\"\n        ],\n        \"hiddenTabSet\": [\n            \"Tutorial\"\n        ],\n        \"console_left_on_top\": false,\n        \"console_right_on_top\": true,\n        \"additional_source_columns\": 0\n    },\n    \"jobs_tab_visibility\": \"shown\",\n    \"code_completion_characters\": 1,\n    \"text_rendering\": \"geometricPrecision\",\n    \"editor_theme\": \"Elm dark {rsthemes}\",\n    \"margin_column\": 96\n}"
  }
]