[
  {
    "objectID": "blog/ggplot2_book_3e_part2.html",
    "href": "blog/ggplot2_book_3e_part2.html",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 2",
    "section": "",
    "text": "1 Introduction\nIn this series of posts, I will be completing the exercises in ggplot2: Elegant Graphics for Data Analysis (3e), the ultimate guide to {ggplot2}. I wanted to practice this textbook to better my knowledge of {ggplot2}, but also get a feel for the design behind the package, The Grammar of Graphics.\nIf you would like to see the source code behind this post, you can click on the Code button at the top of right of the page, sandwiched between the title of the post and the side panel.\nThe book is split into five parts: Getting Started, Layers, Scales, The Grammar, & Advanced Topics. In this post, I will be working through the second part, Layers.\n\n\n\n\n\n\nTip\n\n\n\nBecause this is book about {ggplot2}, I will use package-explicit function when using a function for the first time that is not in base R or provided by {ggplot2}. Most of these packages are loaded with the {tidyverse} meta-package.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe book was still in development when writing this post, so some exercises might not match depending on when you are reading of this post.\n\n\ntest"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Data Science, Technology, & More",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nggplot2: Elegant Graphics for Data Analysis (3e) | Part 2\n\n\nLayers\n\n\n\nR\n\n\n{ggplot2}\n\n\nTextbook Workthrough\n\n\n\nJoin me as I work through the exercises in the textbook.\n\n\n\n\n\nMay 27, 2024\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nggplot2: Elegant Graphics for Data Analysis (3e) | Part 1\n\n\nGetting Started\n\n\n\nR\n\n\n{ggplot2}\n\n\nTextbook Workthrough\n\n\n\nJoin me as I work through the exercises in the textbook.\n\n\n\n\n\nMay 24, 2024\n\n\n23 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ozan Ozbeker",
    "section": "",
    "text": "As a Data Scientist, I specialize in leveraging advanced analytics to tackle complex business challenges. With a solid foundation in Python and SQL, I’ve quickly mastered R and become a key player in my current role, managing significant client projects. My expertise includes developing and maintaining robust data infrastructures with PostgreSQL and enhancing data workflows through efficient ETL processes using R and the Tidyverse. I also have a keen interest in web scraping, utilizing state-of-the-art tools like Helium10, Scrapfly, and Oxylabs to extract actionable insights from major online marketplaces.\nOutside of work, I’m a huge fan of live music, especially Rock and EDM, and I’m making a bigger effort to travel and create experiences this year. I’ve been playing the guitar for about 10 years and creating a budget for around 3 years. When the weather’s nice, I enjoy cycling, hiking, and taking photos. Recently, I’ve developed a keen interest in learning more about the R language outside of my direct work, inspired by the fantastic blogs I’ve come across, which led me to create this website.\nOn this site, you’ll find my resume, personal projects, and a blog where I dive deep into various tools and technologies in the data world, along with other topics I find interesting. Feel free to reach out and connect!"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Summary",
    "section": "",
    "text": "Dynamic and growth-oriented Data Scientist with a background in Manufacturing Engineering and Supply Chain Analytics, I specialize in driving data-centric solutions and innovative projects across diverse industries. Starting with foundational skills in Python and SQL, I quickly became an expert in R, mastering it in real-world scenarios. My role has expanded to being the tech lead for multiple clients, running end-to-end data projects and presenting their results.\n\n\n\n\nData Management & Visualization: Developed robust data infrastructures using self-hosted PostgreSQL and implemented efficient ETL processes with R & the {tidyverse} family of packages, significantly enhancing data accessibility and manipulation. Expert in building insightful Tableau dashboards and Quarto/RMarkdown reports tailored to client needs.\nAdvanced Analytics: Adopted cutting-edge tools like Helium10, Scrapfly, & Oxylabs for sophisticated web scraping, and extracting valuable data from platforms like Amazon and Walmart to inform strategic decisions.\nSoftware Improvement: Adopt the use of Docker, DuckDB, Git, & GitHub, and designed a custom {ggplot2} add-on aligning with company guidelines.\n\nIn my previous roles, I led substantial Continuous Improvement initiatives, generating savings of $1-3 million by optimizing processes on the manufacturing floor. I spearheaded quality control standardization across North American facilities and contributed to inventory management through a bespoke recommendation engine. My technical toolkit expanded during this time to include CAD for designing new machinery layouts and developing ad-hoc dashboards for strategic oversight. My analytical journey is underpinned by a strong educational foundation, including a role as a Teaching Assistant during my undergraduate studies, where I honed my skills in teaching and academic leadership.\n\n\n\nAnalytics: R (Tidyverse), SQL (PostgreSQL, DuckDB), Git & GitHub, Excel\nCommunication: Quarto, ggplot2, gt, shiny, HTML & SCSS/CSS"
  },
  {
    "objectID": "resume.html#data-analytics-consulting",
    "href": "resume.html#data-analytics-consulting",
    "title": "Summary",
    "section": "",
    "text": "Dynamic and growth-oriented Data Scientist with a background in Manufacturing Engineering and Supply Chain Analytics, I specialize in driving data-centric solutions and innovative projects across diverse industries. Starting with foundational skills in Python and SQL, I quickly became an expert in R, mastering it in real-world scenarios. My role has expanded to being the tech lead for multiple clients, running end-to-end data projects and presenting their results."
  },
  {
    "objectID": "resume.html#key-contributions",
    "href": "resume.html#key-contributions",
    "title": "Summary",
    "section": "",
    "text": "Data Management & Visualization: Developed robust data infrastructures using self-hosted PostgreSQL and implemented efficient ETL processes with R & the {tidyverse} family of packages, significantly enhancing data accessibility and manipulation. Expert in building insightful Tableau dashboards and Quarto/RMarkdown reports tailored to client needs.\nAdvanced Analytics: Adopted cutting-edge tools like Helium10, Scrapfly, & Oxylabs for sophisticated web scraping, and extracting valuable data from platforms like Amazon and Walmart to inform strategic decisions.\nSoftware Improvement: Adopt the use of Docker, DuckDB, Git, & GitHub, and designed a custom {ggplot2} add-on aligning with company guidelines.\n\nIn my previous roles, I led substantial Continuous Improvement initiatives, generating savings of $1-3 million by optimizing processes on the manufacturing floor. I spearheaded quality control standardization across North American facilities and contributed to inventory management through a bespoke recommendation engine. My technical toolkit expanded during this time to include CAD for designing new machinery layouts and developing ad-hoc dashboards for strategic oversight. My analytical journey is underpinned by a strong educational foundation, including a role as a Teaching Assistant during my undergraduate studies, where I honed my skills in teaching and academic leadership."
  },
  {
    "objectID": "resume.html#core-competencies",
    "href": "resume.html#core-competencies",
    "title": "Summary",
    "section": "",
    "text": "Analytics: R (Tidyverse), SQL (PostgreSQL, DuckDB), Git & GitHub, Excel\nCommunication: Quarto, ggplot2, gt, shiny, HTML & SCSS/CSS"
  },
  {
    "objectID": "resume.html#data-scientist",
    "href": "resume.html#data-scientist",
    "title": "Summary",
    "section": "Data Scientist",
    "text": "Data Scientist\nRXA @ OneMagnify - Remote, USA | January 2023 - Current\n\nClient Work: Built and maintained: data pipelines, dashboards, & reporting for key stakeholders across multiple clients in the home appliance sector. Sample work includes web scraping, exploratory data analysis, & linear modeling with R; Dashboard & Finance applications with Tableau and Tableau Prep. Heavy focus on Tidyverse & Tidymodels family of R packages, with ad-hoc reporting supported with Quarto/Markdown.\nInternal Projects: Developed new standards for database usage (DuckDB), incorporated Git (GitHub) & Docker into project workflows & created internal R package for meeting company branding guidelines."
  },
  {
    "objectID": "resume.html#continuous-improvement-specialist-oldp-rotation-2",
    "href": "resume.html#continuous-improvement-specialist-oldp-rotation-2",
    "title": "Summary",
    "section": "Continuous Improvement Specialist (OLDP Rotation 2)",
    "text": "Continuous Improvement Specialist (OLDP Rotation 2)\nXylem, Inc - Chicago, IL | July 2022 - December 2022\n\nExcess & Obsolete Inventory Identification & Disposition: Using the DMAIC/DMADV methodology, created a process to reduce the site’s E&O financial reserve and material in our warehouses by identifying items that are obsolete and creating rework, re-sale, & disposal opportunities. The control plan included the creation of a cross-departmental E&O task force, and the new process has estimated hard savings of $1M - $4M in the next 3-5 years.\nAd-Hoc Floor Workshops: Led and assisted many projects to drive Continuous Improvement initiatives at the site, including but not limited to: 5S & Safety audits, lean methodology seminars, inventory audits, & cycle counts."
  },
  {
    "objectID": "resume.html#business-data-analyst-oldp-rotation-1",
    "href": "resume.html#business-data-analyst-oldp-rotation-1",
    "title": "Summary",
    "section": "Business Data Analyst (OLDP Rotation 1)",
    "text": "Business Data Analyst (OLDP Rotation 1)\nXylem, Inc - Uniontown, PA | July 2021 - July 2022\n\nExcess & Obsolete Reserve: Designed a recommendation engine for the quarterly report with an emphasis on reducing ETL time and standardizing the process across all Sensus North America sites to meet new financial policies. The new engine reduced process time by 72 hours (-90%) per year and led to new reporting and forecasting capabilities.\nSensus North America Physical Inventory: Created documentation for the annual NA physical inventory event, including process flowcharts, work instructions, and manager checklists for tasks leading up to the event. The documentation package standardized the process across all NA sites and removed non-value-added activities during the event.\nAd-Hoc Reporting: Created various data pipelines, dashboards, and reports for unique business needs, including tracking forecasting accuracy, comparing supplier and internal bill of materials, gathering and dashboarding quality control measures, and synchronizing lot sizes across multiple sites"
  },
  {
    "objectID": "resume.html#undergraduate-teaching-assistant",
    "href": "resume.html#undergraduate-teaching-assistant",
    "title": "Summary",
    "section": "Undergraduate Teaching Assistant",
    "text": "Undergraduate Teaching Assistant\nWest Virginia University - Morgantown, WV | August 2019 - May 2021\n\nSpring 2021 | IENG 220: Re-engineering Management Systems\nFall 2020 | IENG 305: Introduction to Systems Engineering\nSpring 2020 | IENG 331: Computer Applications in Industrial Engineering\nFall 2019 | IENG 445: Project Management for Engineers"
  },
  {
    "objectID": "resume.html#manufacturing-engineer-intern",
    "href": "resume.html#manufacturing-engineer-intern",
    "title": "Summary",
    "section": "Manufacturing Engineer Intern",
    "text": "Manufacturing Engineer Intern\nJLG Industries - McConnellsburg, PA | June 2019 - August 2019\n\nLong-term Project: Test – Inspect – Green Tag (TIG) Line\n\nConducted a full 5S sweep of the line, removing unnecessary equipment and parts from work stations and laid corners.\nCreated from scratch Standard Work Instructions (SWI) for all 5 stations of the TIG Line for 3 different products.\nImplemented a scanner-based system for entering defects for Electronic Quality Control (EQC), standardizing the process and reducing variability in defect types and improving data collection.\n\nShort-term Project: Tire Manipulator\n\nCreated floor plans for current and future station layouts for the new Tire Manipulator with laser measures and AutoCAD."
  },
  {
    "objectID": "resume.html#bachelor-of-science-in-industrial-engineering",
    "href": "resume.html#bachelor-of-science-in-industrial-engineering",
    "title": "Summary",
    "section": "Bachelor of Science in Industrial Engineering",
    "text": "Bachelor of Science in Industrial Engineering\nWest Virginia University | Morgantown, WV"
  },
  {
    "objectID": "resume.html#certifications",
    "href": "resume.html#certifications",
    "title": "Summary",
    "section": "Certifications",
    "text": "Certifications\n\nLean Six Sigma Green Belt | Institute of Industrial and Systems Engineers\nContinuous Improvement Fundamentals | Oshkosh Corporation\nEligible for Certified Associate in Project Management (CAPM) | Project Management Institute"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html",
    "href": "blog/ggplot2_book_3e_part1.html",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "",
    "text": "In this series of posts, I will be completing the exercises in ggplot2: Elegant Graphics for Data Analysis (3e), the ultimate guide to {ggplot2}. I wanted to practice this textbook to better my knowledge of {ggplot2}, but also get a feel for the design behind the package, The Grammar of Graphics.\n\n“Without a grammar, there is no underlying theory, so most graphics packages are a big collection of special cases.”\n\nI will not be re-iterating all of the information from the book, but provide a brief summary of each section and run through the exercises. Follow along to see my take on the exercises, as well as my notes and thoughts as I progress through the book.\nIf you would like to see the source code behind this post, you can click on the Code button at the top of right of the page, sandwiched between the title of the post and the side panel.\nThe book is split into five parts: Getting Started, Layers, Scales, The Grammar, & Advanced Topics. In this post, I will be working through the first part, Getting Started.\n\n\n\n\n\n\nTip\n\n\n\nBecause this is book about {ggplot2}, I will use package-explicit function when using a function for the first time that is not in base R or provided by {ggplot2}. All of these packages are loaded with the {tidyverse} meta-package.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe book was still in development when writing this post, so some exercises might not match depending on when you are reading of this post.\n\n\n\n\ninstall.packages(c(\n  \"colorBlindness\", \"directlabels\", \"dplyr\", \"ggforce\", \"gghighlight\",\n  \"ggnewscale\", \"ggplot2\", \"ggraph\", \"ggtext\", \"ggthemes\", \"hexbin\", \"Hmisc\", \n  \"mapproj\", \"maps\", \"munsell\", \"ozmaps\", \"paletteer\", \"patchwork\", \"rmapshaper\",\n  \"scico\", \"seriation\", \"sf\", \"stars\", \"tidygraph\", \"tidyr\", \"wesanderson\"\n))"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#required-packages",
    "href": "blog/ggplot2_book_3e_part1.html#required-packages",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "",
    "text": "install.packages(c(\n  \"colorBlindness\", \"directlabels\", \"dplyr\", \"ggforce\", \"gghighlight\",\n  \"ggnewscale\", \"ggplot2\", \"ggraph\", \"ggtext\", \"ggthemes\", \"hexbin\", \"Hmisc\", \n  \"mapproj\", \"maps\", \"munsell\", \"ozmaps\", \"paletteer\", \"patchwork\", \"rmapshaper\",\n  \"scico\", \"seriation\", \"sf\", \"stars\", \"tidygraph\", \"tidyr\", \"wesanderson\"\n))"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#introduction-1",
    "href": "blog/ggplot2_book_3e_part1.html#introduction-1",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.1 Introduction",
    "text": "2.1 Introduction\nThe goal of this chapter it so introduce the reader to {ggplot2} as quickly as possible. Because it’s an intro, I will not be formatting the plots any further than the questions ask for."
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#fuel-economy-data",
    "href": "blog/ggplot2_book_3e_part1.html#fuel-economy-data",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.2 Fuel Economy Data",
    "text": "2.2 Fuel Economy Data\nIn this chapter, we will be using mostly one data set, mpg, from http://fueleconomy.gov. It holds information about the fuel economy of popular car models in 1999 & 2009.\n\nlibrary(tidyverse) # Data Wrangling, includes {ggplot2}\n\nmpg\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\nA quick overview of the variables:\n\n\ncty and hwy record miles per gallon (mpg) for city and highway driving.\n\ndispl is the engine displacement in liters.\n\ndrv is the drivetrain: front wheel (f), rear wheel (r), or four wheel.\n\nmodel is the model of car. There are 38 models, selected because they had a new edition every year between 1999 and 2008.\n\nclass is a categorical variable describing the “type” of car: two seater, SUV, compact, etc.\n\n\n2.2.1 Exercises\n1. List five functions that you could use to get more information about the mpg dataset.\n\nhelp(mpg)\nglimpse(mpg)\nhead(mpg)\nstr(mpg)\nView(mpg)\n\n2. How can you find out what other datasets are included with {ggplot2}?\n\ndata(package = 'ggplot2')\n\n3. Apart from the US, most countries use fuel consumption (fuel consumed over fixed distance) rather than fuel economy (distance traveled with fixed amount of fuel). How could you convert cty and hwy into the European standard of l/100km?\n\nus_to_euro = function(mpg) {\n  # 1 mile = 1.60934 kilometers\n  # 1 gallon (US) = 3.78541 liters\n  \n  g_p_m = 1 / mpg\n  l_p_km = 3.78541 / 1.60934    # we multiply by 100 because it's \"per 100\"\n  l100km = l_p_km * 100 * g_p_m # the denominator 100 cancels right hand 100\n  \n  return(l100km)\n}\n\nmpg |&gt; dplyr::mutate(\n  cty_euro = us_to_euro(cty), \n  hwy_euro = us_to_euro(hwy), \n  .keep = 'used')\n\n# A tibble: 234 × 4\n     cty   hwy cty_euro hwy_euro\n   &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1    18    29     13.1     8.11\n 2    21    29     11.2     8.11\n 3    20    31     11.8     7.59\n 4    21    30     11.2     7.84\n 5    16    26     14.7     9.05\n 6    18    26     13.1     9.05\n 7    18    27     13.1     8.71\n 8    18    26     13.1     9.05\n 9    16    25     14.7     9.41\n10    20    28     11.8     8.40\n# ℹ 224 more rows\n\n\n4. Which manufacturer has the most models in this dataset? Which model has the most variations? Does your answer change if you remove the redundant specification of drivetrain (e.g. “pathfinder 4wd”, “a4 quattro”) from the model name?\nmpg |&gt; dplyr::count(manufacturer)\nmpg |&gt; count(model)\nmpg |&gt; \n  mutate(model_base = stringr::str_extract(model, \"^\\\\w+\")) |&gt; \n  count(model_base) # yes\n\n\n\n# A tibble: 15 × 2\n   manufacturer     n\n   &lt;chr&gt;        &lt;int&gt;\n 1 audi            18\n 2 chevrolet       19\n 3 dodge           37\n 4 ford            25\n 5 honda            9\n 6 hyundai         14\n 7 jeep             8\n 8 land rover       4\n 9 lincoln          3\n10 mercury          4\n11 nissan          13\n12 pontiac          5\n13 subaru          14\n14 toyota          34\n15 volkswagen      27\n\n\n# A tibble: 38 × 2\n   model                  n\n   &lt;chr&gt;              &lt;int&gt;\n 1 4runner 4wd            6\n 2 a4                     7\n 3 a4 quattro             8\n 4 a6 quattro             3\n 5 altima                 6\n 6 c1500 suburban 2wd     5\n 7 camry                  7\n 8 camry solara           7\n 9 caravan 2wd           11\n10 civic                  9\n# ℹ 28 more rows\n\n\n# A tibble: 35 × 2\n   model_base     n\n   &lt;chr&gt;      &lt;int&gt;\n 1 4runner        6\n 2 a4            15\n 3 a6             3\n 4 altima         6\n 5 c1500          5\n 6 camry         14\n 7 caravan       11\n 8 civic          9\n 9 corolla        5\n10 corvette       5\n# ℹ 25 more rows"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#key-components",
    "href": "blog/ggplot2_book_3e_part1.html#key-components",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.3 Key Components",
    "text": "2.3 Key Components\nEvery {ggplot2} plot has three key components:\n\n\nData,\nA set of aesthetic mappings between variables in the data and visual properties, and\nAt least one layer which describes how to render each observation. Layers are usually created with a geom function.\n\nHere’s a simple example:\n\nggplot(mpg, aes(x = displ, y = hwy)) + geom_point()\n\n\n\n\n\n\n\n\n2.3.1 Exercises\n1. How would you describe the relationship between cty and hwy? Do you have any concerns about drawing conclusions from that plot?\nThere is a strong positive linear relationship between city & highway gas mileage. Just plotting only those two might generalize too much across different classes of vehicles. Even though it may be true, maybe different classes of vehicles are more equal in city vs highway gas mileage vs performing substantially better in one or the other.\n2. What does ggplot(mpg, aes(model, manufacturer)) + geom_point() show? Is it useful? How could you modify that data to make it more informative?\n\nggplot(mpg, aes(model, manufacturer)) + geom_point()\n\n\n\n\n\n\n\nThis plot just shows which manufacturers make which models. Having two categorical variables on a dot plot is not very useful as there is no inherent value in the relationship between two categories existing. Turning one category into a count() or other stat would show a dimensional relationship across the other category.\n\nmpg |&gt; count(manufacturer) |&gt; ggplot(aes(n, manufacturer)) + geom_point()\n\n\n\n\n\n\n\n3. Describe the data, aesthetic mappings, and layers used for each of the following plots. You’ll need to guess a little because you haven’t seen all the datasets and functions yet, but use your common sense! See if you can predict what the plot will look like before running the code.\n\n\nggplot(mpg, aes(cty, hwy)) + geom_point() A dot plot showing a positive relationship between city mpg and highway mpg.\n\nggplot(diamonds, aes(carat, price)) + geom_point() A dot plot showing a positive relationship between diamond price and its carat rating.\n\nggplot(economics, aes(date, unemploy)) + geom_line() A line plot showing unemployment rate across time.\n\nggplot(mpg, aes(cty)) + geom_histogram() A histogram showing the distribution of cars across city mpg rating.\n\nggplot(mpg, aes(cty, hwy)) + geom_point()\nggplot(diamonds, aes(carat, price)) + geom_point()\nggplot(economics, aes(date, unemploy)) + geom_line()\nggplot(mpg, aes(cty)) + geom_histogram()"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#color-size-shape-and-other-aesthetic-attributes",
    "href": "blog/ggplot2_book_3e_part1.html#color-size-shape-and-other-aesthetic-attributes",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.4 Color, Size, Shape, and Other Aesthetic Attributes",
    "text": "2.4 Color, Size, Shape, and Other Aesthetic Attributes\nTo add additional variables to a plot, we can use other aesthetics like color, shape, and size. These work in the same way as the x and y aesthetics, and are added into the call to aes():\n\naes(displ, hwy, color = class)\naes(displ, hwy, shape = drv)\naes(displ, hwy, size = cyl)\n\n\nggplot(mpg, aes(displ, hwy, color = class)) + geom_point()\n\n\n\n\n\n\n\n\n2.4.1 Exercises\n1. Experiment with the color, shape and size aesthetics. What happens when you map them to continuous values? What about categorical values? What happens when you use more than one aesthetic in a plot?\nggplot(mpg, aes(displ, hwy, color = cyl)) + geom_point()\nggplot(mpg, aes(displ, hwy, color = as.character(year))) + geom_point()\nggplot(mpg, aes(displ, hwy, color = cyl, shape = as.character(year))) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n2. What happens if you map a continuous variable to shape? Why? What happens if you map trans to shape? Why?\n\nggplot(mpg, aes(displ, hwy, shape = hwy)) + geom_point()\n\nError in `geom_point()`:\n! Problem while computing aesthetics.\nℹ Error occurred in the 1st layer.\nCaused by error in `scale_f()`:\n! A continuous variable cannot be mapped to the shape aesthetic.\nℹ Choose a different aesthetic or use `scale_shape_binned()`.\n\n\nYou get an error because continuous variables lie on a scale of infinity, and you cannot have infinite shapes. This is why in the previous question, converter year into a character because it is a continuous variable year = 1999 in the data frame, but its use is actually as a category, comparing 1999 vehicles to 2008 vehicles.\n\nggplot(mpg, aes(displ, hwy, shape = trans)) + geom_point()\n\nWarning: The shape palette can deal with a maximum of 6 discrete values because more\nthan 6 becomes difficult to discriminate\nℹ you have requested 10 values. Consider specifying shapes manually if you need\n  that many have them.\n\n\nWarning: Removed 96 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nYou get a warning because although trans is a categorical variable, it has more values than {ggplot2} has shapes (6 in total), so other values do not get markers.\nThis highlights the difference between Errors and Warnings with {ggplot2}. As with regular R code, Warnings show where the code can still run but probably with not the effect that was intended, where Errors are impossible to process and the code does not run.\n3. How is drive train related to fuel economy? How is drive train related to engine size and class?\nggplot(mpg, aes(drv, hwy)) + geom_point()\nggplot(mpg, aes(displ, class, color = drv)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nFront-wheel transmission vehicle seems to have the best highway gas mileage, where 4-wheel and rear-wheel show similar performance to each other.\nAlmost all 4-wheel drive vehicles (in this dataset) are either an SUV or Pickup, and have the biggest range in engine size. The smallest vehicles (2-seater & subcompact) have bigger engines and are rear-wheel drive, probably sports cars of some sort. Finally, the regular everyday vehicles like compact & midsize cars have smaller engines and mostly front-wheel drive transmissions."
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#faceting",
    "href": "blog/ggplot2_book_3e_part1.html#faceting",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.5 Faceting",
    "text": "2.5 Faceting\nFaceting creates tables of graphics by splitting the data into subsets and displaying the same graph for each subset. The two type so faceting are grid and wrapped. We will be focusing on wrapped.\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~class)\n\n\n\n\n\n\n\n\n2.5.1 Exercises\n1. What happens if you try to facet by a continuous variable like hwy? What about cyl? What’s the key difference?\nggplot(mpg, aes(displ, cty)) + geom_point() + facet_wrap(~hwy)\nggplot(mpg, aes(displ, cty)) + geom_point() + facet_wrap(~cyl)\n\n\n\n\n\n\n\n\n\n\nThey both facet by the number of unique values in the variable. In the case of hwy, there were 27 unique values in this limited dataset because it truly represents a continuous variable. This makes it a bad choice for faceting.\ncyl on the other hand, actually represents a category (# of cylinders in an engine) although it’s in the data frame as a continuous. This may be because it is a number, which is usually continuous.\n2. Use faceting to explore the 3-way relationship between fuel economy, engine size, and number of cylinders. How does faceting by number of cylinders change your assessment of the relationship between engine size and fuel economy?\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~cyl)\n\n\n\n\n\n\n\nFaceting by cylinders shows a clearly that smaller engines perform better on fuel economy vs their bigger counterparts, though being a smaller engine does not necessarily mean that it will have good gas mileage.\n3. Read the documentation for facet_wrap(). What arguments can you use to control how many rows and columns appear in the output?\nnrow & ncol are the arguments to control the number of rows & columns. Here is an extreme example:\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~trans, nrow = 1)\n\n\n\n\n\n\n\n4. What does the scales argument to facet_wrap() do? When might you use it?\nBy default, the scales locks both the x and y scales on all faceted plots to show the same range, regardless of the range of values in each facet. You might want to free a scale if the axis doesn’t have any values for that facet, and the missing range doesn’t affect the analysis.\nIn this example, we don’t need x-axis values for vehicles that don’t exist, but keeping the y-axis values on the same scale helps to compare the values across all manufacturers.\n\nmpg |&gt; \n  ggplot(aes(as_factor(cyl), hwy)) + \n  geom_point() + \n  facet_wrap(~manufacturer, scales = \"free_x\", nrow = 3)"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#plot-geoms",
    "href": "blog/ggplot2_book_3e_part1.html#plot-geoms",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.6 Plot Geoms",
    "text": "2.6 Plot Geoms\nSubstituting geom_point() for a different geom function creates a different plot. Who would’ve thought? In the following sections, we will cover some of the other most used geoms provided in {ggplot2}:\n\n\ngeom_smooth() fits a smoother to the data and displays the smooth and its standard error.\n\ngeom_boxplot() produces a box-and-whisker plot to summarize the distribution of set of points.\n\ngeom_histogram() and geom_freqpoly() show the distribution of continuous variables.\n\ngeom_bar() shows the distribution of categorical variables.\n\ngeom_path() and geom_line() draw lines between the data points. A line plot is constrained to produce lines that travel from left to right, while paths can go in any direction. Lines are typically used to explore how things change over time.\n\n\n2.6.1 Adding a smoother to a plot\nIf you have a scatterplot with a lot of noise, it can be hard to see the dominant pattern. In this case, it’s useful to add a smoothed line to the plot with geom_smooth():\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nAn important argument to geom_smooth() is the method, which allows you to choose which type of model is used to fit the smooth curve:\n\n\nmethod = \"loess\", the default for small n, uses a smooth local regression. \"span\" controls the level of smoothing.\n\nmethod = \"gam\" fits a generalized additive model provided by the {mgcv} package. You need to load in the package then use a formula = y ~ s(x) or y ~ s(x, bs = \"cs\") (for large data).\n\nmethod = \"lm\" fits a linear model, giving the line of best fit.\n\nggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(span = 0.2)\nggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(span = 1)\nlibrary(mgcv)\nggplot(mpg, aes(displ, hwy)) + \n  geom_point() + \n  geom_smooth(method = \"gam\", formula = y ~ s(x))\nggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 Boxplots and jittered points\nWhen a dataset contains a categorical variable and one or more continuous variables, we might be interested in the distribution of the continuous variable(s) relative to the categorical variable. Because there a few unique number of values for both drv and hwy, there is a lot of overplotting. There are a few useful techniques to help with this issue:\nggplot(mpg, aes(drv, hwy)) + geom_point()\nggplot(mpg, aes(drv, hwy)) + geom_jitter()\nggplot(mpg, aes(drv, hwy)) + geom_boxplot()\nggplot(mpg, aes(drv, hwy)) + geom_violin()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThough these are useful techniques, they also have their own drawbacks.\n\n\n\n\n\n\n\nJitter Plots\nBoxplots\nViolin Plots\n\n\n\nAdd a little random noise to the data which can help avoid overplotting.\nSummarize the shape of the distribution with a handful of summary statistics.\nShow a compact representation of the “density” of the distribution, highlighting the areas where more points are found.\n\n\nShow every point but only work with relatively small datasets.\nSummarize the whole distribution with 5 statistics.\nGive the richest display, but the density estimate can be hard to interpret.\n\n\n\n2.6.3 Histograms and frequency polygons\nHistograms and frequency polygons show the distribution of a single numeric variable with more detail than a boxplot but at the expense of needing more space. The only difference between the two is that the prior uses columns and the latter uses lines.\nggplot(mpg, aes(hwy)) + geom_histogram()\nggplot(mpg, aes(hwy)) + geom_freqpoly()\n\n\n\n\n\n\n\n\n\n\nIt is highly recommended to experiment with the bins as the default value is 30 and it is unlikely that 30 is the best choice for your dataset.\nggplot(mpg, aes(hwy)) + geom_freqpoly(bins = 15)\nggplot(mpg, aes(hwy)) + geom_freqpoly(bins = 45)\n\n\n\n\n\n\n\n\n\n\nTo compare distributions of subgroups, you can map a categorical variable to either fill (for geom_histogram()) or color (for geom_freqpoly()).\nggplot(mpg, aes(displ, colour = drv)) + \n  geom_freqpoly(binwidth = 0.5)\nggplot(mpg, aes(displ, fill = drv)) + \n  geom_histogram(binwidth = 0.5) + \n  facet_wrap(~drv, ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n2.6.4 Bar charts\nThe discrete analogue of the histogram is the bar chart, geom_bar().\n\nggplot(mpg, aes(manufacturer)) + geom_bar()\n\n\n\n\n\n\n\nBar charts can be confusing because there are two very different plots that are both commonly called bar charts.\n\nThe first form, like above, assumes your data is not summarized, and each observation contributes to one unit to the height of each bar.\nThe second form is used for pre-summarized data.\n\nFor example, you might have three drugs with the their average effect. To display this type of data, you have to tell geom_bar() to not run the default stat which bins and counts data. In this case, it’s better to use geom_point() because it takes up less space than bars, and don’t require that the axis includes 0.\ndrugs = tibble(drug = c(\"a\", \"b\", \"c\"), effect = c(4.2, 9.7, 6.1))\n\nggplot(drugs, aes(drug, effect)) + geom_bar(stat = \"identity\")\nggplot(drugs, aes(drug, effect)) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n2.6.4.1 Bonus\nEven if using geom_point() might be preferred, because the second type of bar/column chart is so popular, {ggplot2} includes a geom_col() that acts exactly the same as geom_bar(stat = \"identity\"):\nggplot(drugs, aes(drug, effect)) + geom_bar(stat = \"identity\")\nggplot(drugs, aes(drug, effect)) + geom_col()\n\n\n\n\n\n\n\n\n\n\n\n2.6.5 Time series with line and path plots\nLine and path plots are typically used for time series data, where the order of the data matters to the context of the visual. Line plots join the data points from left to right, while path plots join the points in the order that they appear in the dataset.\n\n\n\n\n\n\nLine Plot\nPath Plot\n\n\n\nPlots the data from left to right.\nPlots the points in the order they appear in dataset.\n\n\nShow time on x-axis.\nShow how two variables simultaneously change over time, with time encoded in the way the data points are connected.\n\n\n\nThe two plots below show unemployment over time, both with geom_line(). The firsts shows unemployment rate while the second shows the median number of weeks unemployed.\nggplot(economics, aes(date, unemploy / pop)) + geom_line()\nggplot(economics, aes(date, uempmed)) + geom_line()\n\n\n\n\n\n\n\n\n\n\nTo compare the relationship, we would like to draw both time series on the same plot. We could draw a scatterplot of unemployment rate vs length of time unemployed, but then we lose the dimension of time. The solution is to join points adjacent in time with line segments, forming a path plot.\nggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path() +\n  geom_point()\nggplot(economics, aes(unemploy / pop, uempmed)) + \n  geom_path(color = \"grey50\") +\n  geom_point(aes(color = lubridate::year(date)))\n\n\n\n\n\n\n\n\n\n\n\n2.6.6 Exercises\n\n\n\n\n\n\nNote\n\n\n\nGoing through the exercises of this section, they don’t all completely line up with content above, so I believe this part is still a WIP. Regardless, I tried to answered the questions to the best of what I think the exercises are going for.\n\n\n1. What’s the problem with the plot created by ggplot(mpg, aes(cty, hwy)) + geom_point()? Which of the geoms described above is most effective at remedying the problem?\nBecause the two values are so highly correlated, there might be some overplotting in the plot. We can check for overplotting imonn a scatterplot by adjust the alpha value of the points.\nggplot(mpg, aes(cty, hwy)) + geom_point()\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 0.2)\n\n\n\n\n\n\n\n\n\n\n2. One challenge with ggplot(mpg, aes(class, hwy)) + geom_boxplot() is that the ordering of class is alphabetical, which is not terribly useful. How could you change the factor levels to be more informative? Rather than reordering the factor by hand, you can do it automatically based on the data: ggplot(mpg, aes(reorder(class, hwy), hwy)) + geom_boxplot(). What does reorder() do? Read the documentation.\nggplot(mpg, aes(class, hwy)) + geom_boxplot()\nggplot(mpg, aes(\n  x = factor(class, levels = c(\n    \"pickup\", \"suv\", \"minivan\", \"2seater\", \"subcompact\", \"compact\", \"midsize\")), \n  y = hwy)) + \n  geom_boxplot()\nggplot(mpg, aes(reorder(class, hwy), hwy)) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\nreorder() treats its first argument as a categorical variable, and reorders its levels based on the values of a second variable, usually numeric. Comparing my manual reordering (based on the median line of the boxplots), reorder() takes a different approach that doesn’t look apparently clear from the boxplot.\n3. Explore the distribution of the carat variable in the diamonds dataset. What binwidth reveals the most interesting patterns?\nYou can use either bin or binwidth to distribute the columns of a histogram. You can test bin values arbitrarily until you find a plot that looks good, but using the binwidth argument, you can make educated guess as to where to begin and go because the value is directly related to the value/scaling of the x-axis.\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 1/10)\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 1/50) \nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 1/100)\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 1/500)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing binwidth values of 1/50 & 1/100 shows that the caret values heavily sit on half and quarter caret values, skewing right until it breaks again.\n4. Explore the distribution of the price variable in the diamonds data. How does the distribution vary by cut?\nggplot(diamonds, aes(price)) + geom_freqpoly()\nggplot(diamonds, aes(price)) + geom_freqpoly(aes(color = cut))\n\n\n\n\n\n\n\n\n\n\nThe regular and segmented plots both show a similar distribution. I’m not surprised that higher value cuts sell more, but I am surprised that all the cuts sell the most at the same price range. I would’ve thought that higher value cuts would seller at higher prices more frequently.\n5. You now know (at least) three ways to compare the distributions of subgroups: geom_violin(), geom_freqpoly(), and the color aesthetic, or geom_histogram() and faceting. What are the strengths and weaknesses of each approach? What other approaches could you try?\nThe strengths and weakness were outlined pretty well above, but you could also try adjust with the transparency of the data points, which also can be used with two sets of continuous variables.\nggplot(mpg, aes(drv, hwy)) + geom_point(alpha = 0.2) # Category vs Continuous\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 0.2) # Continuous vs Continuous\n\n\n\n\n\n\n\n\n\n\n6. Read the documentation for geom_bar(). What does the weight aesthetic do?\nThe weight argument in the geom_bar() function allows you to adjust the heights of the bars according to the weights of the observations rather than simply counting the number of observations. This can be useful when you have a frequency or probability data set and you want to visualize it accurately.\ndata = tibble(\n  category = c(\"A\", \"A\", \"B\", \"B\", \"B\", \"C\", \"C\"),\n  weight = c(1, 2, 1, 3, 1, 2, 1)\n)\n\n# Plot without weights\nggplot(data, aes(category)) + geom_bar()\n# Plot with weights\nggplot(data, aes(category, weight = weight)) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n7. Using the techniques already discussed in this chapter, come up with three ways to visualize a 2d categorical distribution. Try them out by visualizing the distribution of model and manufacturer, trans and class, and cyl and trans.\n\nggplot(mpg, aes(manufacturer, model)) + geom_point(aes(size = hwy))\n\n\n\n\n\n\nggplot(mpg, aes(class, fill = trans)) + geom_bar()\n\n\n\n\n\n\nggplot(mpg, aes(cyl)) + geom_bar() + facet_wrap(~trans, nrow = 2)\n\n\n\n\n\n\n\nNot the most insightful plots in the world, but the restrictions were met :)"
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#modifying-the-axes",
    "href": "blog/ggplot2_book_3e_part1.html#modifying-the-axes",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.7 Modifying the Axes",
    "text": "2.7 Modifying the Axes\nThere are two families of useful helpers that let you make the most common modifications.\n\n\nxlab() and ylab() modify the x-axis and y-axis labels:\n\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3)\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) + \n  xlab(\"city driving (mpg)\") + ylab(\"highway driving (mpg)\")\n# Remove the axis labels with NULL\nggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) + xlab(NULL) + ylab(NULL)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nxlim() and ylim() modify the limits of the axes:\n\nggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25)\nggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25) + xlim(\"f\", \"r\") + ylim(20, 30)\n# For continuous scales, use NA to set only one limit\nggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25, na.rm = TRUE) + ylim(NA, 30)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nChanging the axes limits sets values outside the range to NA before it calculates summary statistics. You may use na.rm = TRUE to filter out the new NA values, but it is important to understand the order of operations."
  },
  {
    "objectID": "blog/ggplot2_book_3e_part1.html#output",
    "href": "blog/ggplot2_book_3e_part1.html#output",
    "title": "ggplot2: Elegant Graphics for Data Analysis (3e) | Part 1",
    "section": "\n2.8 Output",
    "text": "2.8 Output\nPlots are usually generated to view immediately, but they can be saved to a variable:\n\np = ggplot(mpg, aes(displ, hwy, color = factor(cyl))) + geom_point()\n\nOnce you have a plot object, you can do a variety of things with it:\n\nRender it on screen with print() (this happens automatically when running interactively, but needs to be called explicitly in a loop or function):\n\n\nprint(p)\n\n\n\n\n\n\n\n\nSave it to disc with ggsave():\n\n\nggsave(\"plot.png\", p, width = 5 , height = 5)\n\n\nBriefly describe its structure with summary():\n\n\nsummary(p)\n\ndata: manufacturer, model, displ, year, cyl, trans, drv, cty, hwy, fl,\n  class [234x11]\nmapping:  x = ~displ, y = ~hwy, colour = ~factor(cyl)\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n-----------------------------------\ngeom_point: na.rm = FALSE\nstat_identity: na.rm = FALSE\nposition_identity"
  }
]