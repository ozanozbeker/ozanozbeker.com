---
format:
  html: 
    toc: true
    toc-title: "Sections"
    other-links:
      - text: "Cover Letter PDF"
        href: resources/documents/Ozan Ozbeker Cover Letter.pdf
        icon: file-earmark-text
      - text: "Resume PDF"
        href: resources/documents/Ozan Ozbeker Resume.pdf
        icon: file-earmark-text
      - text: "GitHub"
        href: https://github.com/ozanozbeker/
        icon: github
      - text: "LinkedIn"
        href: https://www.linkedin.com/in/ozanozbeker/
        icon: linkedin
      - text: "Email"
        href: mailto:ozanozbeker99@outlook.com
        icon: envelope
page-layout: full
---

# Summary {#sec-summary .title}

## Data Analytics & Consulting

As a data science consultant, I have managed end-to-end client projects in the replacement parts business, supporting key decision-making processes. This involved thorough requirements gathering, data integration from multiple sources, and the creation of insightful reports and dashboards to enhance decision-making and strategy formulation. Additionally, I automated internal workflows with SQL queries and formed cross-functional teams to assess and manage SKUs, ensuring effective inventory management through monthly reviews and automated reports.

In previous roles, I have focused extensively on Excess and Obsolete (E&O) inventory, developing automated workflows and comprehensive SQL queries to analyze and manage inventory. This includes automating the ETL process to align with company policies. My role has involved significant cross-departmental collaboration, working with finance, procurement, logistics, sustaining engineering, and product management to establish standardized workflows and create teams for monthly reviews of E&O inventory.

I have extensive experience presenting to clients. This includes managing comprehensive data projects from requirements gathering to presenting results, conducting detailed client interviews, and delivering clear, actionable presentations. Internally, I have worked closely with various business functions to meet data pipeline and dashboard requirements, presenting findings to Director and VP-level stakeholders.

## Core Competencies

::: panel-tabset
## R Programming

I am highly proficient in R within a professional setting, with a strong emphasis on the Tidyverse, Tidymodels, and related packages. My role as a "full-stack" data scientist involves managing the entire data science process framework, encompassing Import, Tidy, Transform, Visualize, Model, and Communicate stages.

### Key Areas of Expertise

1. **Import (& Data Management):**
   - **Data Sources:** Extensive experience importing from databases, spreadsheets, web scraping, and APIs.
   - **Database Connections:** Accessing client data through SQL Server, PostgreSQL, and SAP HANA.
   - **Web Scraping and Automation:** Creating and automating scrapes and API calls for data collection from services like Amazon and Walmart. Using Selenium to automate browsers and bypass bot detection.
   - **Recent Tools:** Working with DuckDB for local data analysis and storage, though it is less frequently needed due to ample memory on work servers.
   - **Data Management:** Managing PostgreSQL databases for clients, including data modeling and management.
   - **Key Tools:** {httr2}, {rvest}, {RSelenium}, Selenium, Oxylabs, Helium10, {DuckDB}, {Rpostgres}, {DBI}, {odbc}

2. **Tidy & Transform:**
   - **Data Wrangling:** Extensive experience in focusing on grouping, filtering, and summarizing data.
   - **Data Tidying:** Transforming data by pivoting and converting data types (e.g., strings to dates, floats).
   - **Regex Expertise:** Skilled in using regular expressions for data cleaning and manipulation.
   - **Key Tools:** {tidyverse}, {janitor}

3. **Visualize:**
   - **Exploratory Data Analysis:** Frequently use {ggplot2} and its extensions for data visualization.
   - **Client Preferences:** While initial exploration and cleaning are done in R, clients prefer Tableau or other BI tools for final dashboards.
   - **Key Tools:** {ggplot2}, {gganimate}, {ggthemes}, {ggraph}.

4. **Model:**
   - **Forecasting and Classification:** Professional experience in forecasting with linear regression models and classification with support vector machines and recommendation engines for pricing.
   - **Key Tools:** Base R Stats, {tidymodels}.

5. **Communicate:**
   - **Automated Reports:** Building automated reports for clients to support financial decision-making, including dashboards and executive reports.
   - **Customization:** Using HTML and CSS to customize and enhance the visual appeal of reports.
   - **Key Tools:** Quarto, {gt}, {ggplot2}, {htmlwidgets}, {htmltools}.

6. **Programming:**
   - **Functional Programming:** Using the {purrr} package for mapping functions and other utilities within the Tidyverse framework.
   - **Version Control:** Incorporating Git and GitHub into workflows and managing project structure.
   - **Key Tools:** Task Scheduler, Git for Windows, GitHub, {fs}.

## Tableau, Tableau Prep, & Power BI

### Tableau Experience

I have developed a strong competency in Tableau through both formal education and practical application in professional settings. Here is a detailed summary of my Tableau experience:

- **Learning and Initial Skills Development:**
  - Gained foundational knowledge of Tableau through DataCamp's introductory course, which covered the most common functions and features.
  - Acquired skills to create various charts and dashboards tailored to managerial needs.

- **Practical Applications:**
  - **Dashboard Creation:**
    - Created dashboards linked to the company's ERP system, primarily displaying aggregated data for daily, weekly, and monthly performance metrics.
    - Developed dashboards for clients that connect to their specific data sources, published on their Tableau Servers. These dashboards cover:
      - Parts distribution across distributors.
      - Performance metrics for Amazon marketplace.
      - Internal supply chain performance monitoring.
    - Utilized a mix of database connections, custom ETL processes, and multifaceted dashboards combining tables and charts for comprehensive stakeholder reporting.

  - **Tableau Prep for ETL:**
    - Used Tableau Prep for ETL tasks to prepare data for analysis and visualization in Tableau. Despite finding Tableau Prep to be slow, this experience motivated me to learn SQL for more efficient data processing.

- **Advanced Tableau Functions:**
  - Maintained multiple dashboards with regular updates and enhancements based on client feedback and evolving data needs.
  - Ensured data integrity and accuracy through rigorous testing and validation of ETL processes and dashboard outputs.

### Power BI Experience

My experience with Power BI stems from a company-wide transition from Tableau to Power BI, where I acquired the basics and applied them in a professional context. Here is a detailed summary of my Power BI experience:

- **Learning and Initial Skills Development:**
  - Gained foundational knowledge of Power BI during a transition from Tableau to Power BI as part of a Microsoft 365 (M365) integration at my previous company.
  - Learned the basics of creating visualizations, reports, and dashboards within Power BI.

- **Practical Applications:**
  - Applied Power BI skills to create and maintain reports and dashboards, similar to those previously developed in Tableau.
  - Utilized Power BI's features to connect to various data sources, perform data transformations, and build interactive visualizations.

## SQL & Python

### SQL Professional Experience
I have significant professional experience in creating complex SQL queries to generate custom reports for internal customers. This involves:

- **Complex Queries:**
  - **Common Table Expressions (CTEs):** Utilizing CTEs for better readability and modular query design.
  - **Subqueries:** Embedding subqueries within main queries for dynamic data retrieval.
  - **Nested Case-When Statements:** Implementing conditional logic within queries to handle complex data manipulation.
  - **Aggregate Calculations:** Performing aggregations such as SUM, AVG, COUNT, etc., to summarize data.
  - **Filtering:** Applying WHERE clauses and HAVING conditions to filter data as per requirements.
  - **Date/Time Functions:** Manipulating and formatting date and time data.
  - **String Functions:** Using functions like CONCAT, SUBSTRING, and LENGTH for string manipulation.

The most challenging aspects of reporting often involve not the SQL code itself but the process of identifying and validating the required data sources. Ensuring data accuracy and consistency is paramount in delivering reliable reports.

#### Continued Education and Advanced Skills

In my continuous pursuit of knowledge and skill enhancement, I have learned and practiced several advanced SQL techniques, including:

- **Window Functions:** Applying functions like ROW_NUMBER, RANK, and LEAD/LAG to perform calculations across specific partitions of data.
- **Transactions and Error Handling:** Implementing transaction control (BEGIN, COMMIT, ROLLBACK) and error handling mechanisms to ensure data integrity.
- **Arrays:** Using arrays for more complex data structures and operations.
- **Pivoting Tables:** Creating pivot tables using the CROSSTAB function to transform data from rows to columns.
- **Creating Totals and Grand Totals:**
  - **ROLLUP:** Generating subtotals and grand totals in query results.
  - **CUBE:** Extending ROLLUP to include cross-tabulated totals.

#### SQL Varieties and Environments

- **PostgreSQL:**
  - Most of my SQL experience is in PostgreSQL, where I have developed and executed a wide range of queries and scripts.
  
- **Other SQL Environments:**
  - **MS Access:** Occasionally used the SQL editor in MS Access for database management tasks.
  - **SQL Server:** Completed a formal class in school focused on SQL Server, gaining foundational knowledge and skills in this environment.

### Python Teaching Experience

- **Undergraduate Teaching Assistant:**
  - While I haven't used Python in a professional setting, my proficiency in Python was recognized when I served as an undergraduate teaching assistant. In this role, I:
    - Assisted students in understanding and applying Python for data analysis and visualization.
    - Helped design and grade assignments, projects, and exams.
    - Conducted review sessions and provided one-on-one tutoring to students.
  - Although I haven't used Python extensively in a professional setting recently, my foundational skills remain strong. I have a solid understanding of the core libraries and techniques needed for data manipulation, analysis, and visualization.

#### Key Python Skills

- **Pandas:** Data manipulation, cleaning, aggregation, and merging.
- **NumPy:** Numerical computations, array manipulation, and mathematical functions.
- **Matplotlib:** Creating static visualizations, customizing plots, and adjusting aesthetics.
- **Seaborn:** Advanced statistical visualizations, integrating with Matplotlib, and enhancing visual appeal.

## Excel

### Basic and Advanced Functionalities

-   **Basic Functionalities:**
    -   Creating tables and charts.
    -   Implementing conditional formatting.
    -   Using nested IF statements for complex logical operations.
-   **Advanced Functions:**
    -   **XLOOKUP:** Efficiently searching for and retrieving data across large datasets.
    -   **COUNTIF:** Counting cells that meet specific criteria.
    -   **IFERROR:** Handling errors gracefully in formulas.
    -   **SUMPRODUCT:** Performing array calculations for more complex data analysis.
    -   **Data Validation:** Ensuring data integrity and consistency within spreadsheets.

### Pivot Tables and Pivot Charts

I am highly proficient in using Pivot Tables and Pivot Charts, which are essential for summarizing, analyzing, and visualizing data. These tools have been integral in my professional work for performing descriptive analytics, such as analyzing inventory levels, past statuses, and conducting ABC analyses to identify trends in usage.

### Power Query for ETL

I frequently utilize Power Query for Extract, Transform, Load (ETL) functions, which allows me to automate data processing workflows. Power Query's capabilities enable me to: 

- Extract data from various sources. 
- Transform and clean the data. 
- Load the processed data into Excel for analysis.

This automation significantly enhances efficiency and accuracy in data handling.

### Professional Applications

In a professional setting, I have applied my Excel skills primarily for: 

- Descriptive analytics related to inventory management. 
- Conducting ABC analyses to classify inventory based on importance. 
- Identifying trends and patterns in data usage.

### Academic Projects

In school, I designed an ERP system within Excel for a small business simulation. This project involved: 

- Creating Bills of Materials (BOM) for two products. 
- Forecasting sales using dummy data. 
- Calculating safety stock levels. 
- Setting product prices based on estimated labor and material costs.

#### A note on VBA and Macros

While I have dabbled in VBA for creating macros, I prefer using standard Excel functions for most tasks to ensure that my tools are user-friendly and easily accessible to others. My experience with working and sharing files in Excel within Teams (formerly known as SharePoint) has further reduced the need for macros, as the collaborative environment supports most of my ETL requirements through Power Query.
:::

# Experience {#sec-experience .title}

## Data Scientist

[**RXA \@ OneMagnify**](https://rxa.io/) **- Remote, USA** \| January 2023 - Current

-   **Client Work**: Built and maintained: data pipelines, dashboards, & reporting for key stakeholders across multiple clients in the home appliance sector. Sample work includes web scraping, exploratory data analysis, & linear modeling with R; Dashboard & Finance applications with Tableau and Tableau Prep. Heavy focus on Tidyverse & Tidymodels family of R packages, with ad-hoc reporting supported with Quarto/Markdown.

-   **Internal Projects**: Developed new standards for database usage (DuckDB), incorporated Git (GitHub) & Docker into project workflows & created internal R package for meeting company branding guidelines.

## Continuous Improvement Specialist (OLDP Rotation 2)

[**Xylem, Inc**](https://www.xylem.com/en-us/) **- Chicago, IL** \| July 2022 - December 2022

-   **Excess & Obsolete Inventory Identification & Disposition:** Using the DMAIC/DMADV methodology, created a process to reduce the site’s E&O financial reserve and material in our warehouses by identifying items that are obsolete and creating rework, re-sale, & disposal opportunities. The control plan included the creation of a cross-departmental E&O task force, and the new process has estimated hard savings of \$1M - \$4M in the next 3-5 years.

-   **Ad-Hoc Floor Workshops:** Led and assisted many projects to drive Continuous Improvement initiatives at the site, including but not limited to: 5S & Safety audits, lean methodology seminars, inventory audits, & cycle counts.

## Business Data Analyst (OLDP Rotation 1)

[**Xylem, Inc**](https://www.xylem.com/en-us/) **- Uniontown, PA** \| July 2021 - July 2022

-   **Excess & Obsolete Reserve:** Designed a recommendation engine for the quarterly report with an emphasis on reducing ETL time and standardizing the process across all Sensus North America sites to meet new financial policies. The new engine reduced process time by 72 hours (-90%) per year and led to new reporting and forecasting capabilities.

-   **Sensus North America Physical Inventory:** Created documentation for the annual NA physical inventory event, including process flowcharts, work instructions, and manager checklists for tasks leading up to the event. The documentation package standardized the process across all NA sites and removed non-value-added activities during the event.

-   **Ad-Hoc Reporting:** Created various data pipelines, dashboards, and reports for unique business needs, including tracking forecasting accuracy, comparing supplier and internal bill of materials, gathering and dashboarding quality control measures, and synchronizing lot sizes across multiple sites

## Undergraduate Teaching Assistant

[**West Virginia University**](https://www.wvu.edu/) **- Morgantown, WV** \| August 2019 - May 2021

-   **Spring 2021** \| IENG 220: Re-engineering Management Systems

-   **Fall 2020** \| IENG 305: Introduction to Systems Engineering

-   **Spring 2020** \| IENG 331: Computer Applications in Industrial Engineering

-   **Fall 2019** \| IENG 445: Project Management for Engineers

## Manufacturing Engineer Intern

[**JLG Industries**](https://www.jlg.com/) **- McConnellsburg, PA** \| June 2019 - August 2019

-   **Long-term Project: Test – Inspect – Green Tag (TIG) Line**

    -   Conducted a full 5S sweep of the line, removing unnecessary equipment and parts from work stations and laid corners.

    -   Created from scratch Standard Work Instructions (SWI) for all 5 stations of the TIG Line for 3 different products.

    -   Implemented a scanner-based system for entering defects for Electronic Quality Control (EQC), standardizing the process and reducing variability in defect types and improving data collection.

-   **Short-term Project:** **Tire Manipulator**

    -   Created floor plans for current and future station layouts for the new Tire Manipulator with laser measures and AutoCAD.

# Education {#sec-education .title}

## Bachelor of Science in Industrial Engineering

**West Virginia University** \| Morgantown, WV

## Certifications

-   **Lean Six Sigma Green Belt** \| Institute of Industrial and Systems Engineers

-   **Continuous Improvement Fundamentals** \| Oshkosh Corporation

-   **Eligible for Certified Associate in Project Management (CAPM)** \| Project Management Institute
